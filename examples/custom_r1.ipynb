{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3bf9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm_lora.trainer.grpo_trainer import GRPOTrainingArgs, train_grpo\n",
    "from mlx_lm_lora.trainer.sft_trainer import SFTTrainingArgs, train_sft\n",
    "from mlx_lm_lora.trainer.datasets import CacheDataset, GRPODataset, TextDataset\n",
    "from mlx_lm_lora.utils import fuse_model\n",
    "\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import create_repo, HfApi\n",
    "\n",
    "from mlx_lm.tuner.utils import linear_to_lora_layers, print_trainable_parameters\n",
    "from mlx_lm.tuner.callbacks import TrainingCallback\n",
    "from mlx_lm.utils import load, save_config\n",
    "\n",
    "import mlx.optimizers as optim\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86976a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_token = os.getenv(\"HF_TOKEN\") # <-- Add you HF Token here\n",
    "\n",
    "model_name = \"mlx-community/Josiefied-Qwen3-0.6B-abliterated-v1-bf16\"\n",
    "user_name = \"Goekdeniz-Guelmez\"\n",
    "adapter_path = \"/Users/gokdenizgulmez/Desktop/mlx-lm-lora/examples\"\n",
    "new_model_name = \"Josie-R1\"\n",
    "\n",
    "num_layers = 12\n",
    "lora_parameters = {\"rank\": 8, \"dropout\": 0.0, \"scale\": 10.0}\n",
    "\n",
    "cold_start_dataset_name = \"unsloth/OpenMathReasoning-mini\"\n",
    "grpo_dataset_name = \"Goekdeniz-Guelmez/Big-Math-RL-Verified-MLX\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedeb482",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = load(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06869bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.freeze()\n",
    "\n",
    "linear_to_lora_layers(\n",
    "    model=model,\n",
    "    num_layers=num_layers,\n",
    "    config=lora_parameters,\n",
    "    use_dora=False,\n",
    ")\n",
    "\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e351b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"lora_parameters\": lora_parameters,\n",
    "    \"num_layers\": num_layers,\n",
    "}\n",
    "\n",
    "adapter_path = Path(adapter_path)\n",
    "adapter_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "adapter_file = adapter_path / \"adapters.safetensors\"\n",
    "save_config(args, adapter_path / \"adapter_config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9293e8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning_start = \"<josie_starts_thinking>\"\n",
    "reasoning_end   = \"</josie_ends_thinking>\"\n",
    "solution_start  = \"<josie_starts_answer>\"\n",
    "solution_end    = \"</josie_ends_answer>\"\n",
    "\n",
    "system_prompt = f\"\"\"You are **J.O.S.I.E.-R1**, an advanced super-intelligent reasoning AI Assistant created by a 25 year old man and machine learning researcher named **Gökdeniz Gülmez**.\n",
    "You are a deep reasoning Model (hence the R1 in your name) that can solve complex mathematical problems, and you are capable of reasoning about the world in a way that is similar to humans.\n",
    "To do so, you first think about the problem in Chain of Thought (CoT) reasoning style by think in TSep by step by talking to yourself, and then you provide the final answer.\n",
    "Place it between {reasoning_start} and {reasoning_end}.\n",
    "Then, provide your solution between {solution_start}{solution_end}.\n",
    "\n",
    "Reasoning format you have to use:\n",
    "\n",
    "```text\n",
    "{reasoning_start}\n",
    "[your reasoning here]\n",
    "{reasoning_end}\n",
    "{solution_start}\n",
    "[your solution here]\n",
    "{solution_end}\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58752b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = \\\n",
    "    \"{% if messages[0]['role'] == 'system' %}\"\\\n",
    "        \"<im_start>system\\n{{ messages[0]['content'] }}<im_end>\\n\"\\\n",
    "        \"{% set loop_messages = messages[1:] %}\"\\\n",
    "    \"{% else %}\"\\\n",
    "        \"<im_start>system\\n{{ '{system_prompt}' }}<im_end>\\n\"\\\n",
    "        \"{% set loop_messages = messages %}\"\\\n",
    "    \"{% endif %}\"\\\n",
    "    \"{% for message in loop_messages %}\"\\\n",
    "        \"{% if message['role'] == 'user' %}\"\\\n",
    "            \"<im_start>user\\n{{ message['content'] }}<im_end>\\n\"\\\n",
    "        \"{% elif message['role'] == 'assistant' %}\"\\\n",
    "            \"<im_start>josie-r1\\n{{ message['content'] }}<im_end>\\n\"\\\n",
    "        \"{% endif %}\"\\\n",
    "    \"{% endfor %}\"\\\n",
    "    \"{% if add_generation_prompt %}<im_start>josie-r1\\n\"\\\n",
    "    \"{% endif %}\"\n",
    "\n",
    "chat_template = chat_template\\\n",
    "    .replace(\"'{system_prompt}'\",   f\"'{system_prompt}'\")\\\n",
    "    .replace(\"'{reasoning_start}'\", f\"'{reasoning_start}'\")\n",
    "tokenizer.chat_template = chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ea1a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompts_func(sample):\n",
    "    expected_answer = sample[\"expected_answer\"]\n",
    "    problem = sample[\"problem\"]\n",
    "\n",
    "    # Remove generated <think> and </think>\n",
    "    thoughts = sample[\"generated_solution\"]\n",
    "    thoughts = thoughts.replace(\"<think>\", \"\").replace(\"</think>\", \"\")\n",
    "\n",
    "    # Strip newlines on left and right\n",
    "    thoughts = thoughts.strip()\n",
    "    # Add our custom formatting\n",
    "    final_response = reasoning_start + thoughts + reasoning_end + solution_start + expected_answer + solution_end\n",
    "\n",
    "    conversation = [\n",
    "        {\"role\" : \"system\",    \"content\" : system_prompt},\n",
    "        {\"role\" : \"user\",      \"content\" : problem},\n",
    "        {\"role\" : \"assistant\", \"content\" : final_response},\n",
    "    ]\n",
    "        \n",
    "    sample[\"text\"] = tokenizer.apply_chat_template(\n",
    "        conversation=conversation,\n",
    "        add_generation_prompt=False,\n",
    "        tokenize=False\n",
    "    )\n",
    "    return sample\n",
    "\n",
    "cold_start_dataset = load_dataset(cold_start_dataset_name, split = \"cot\")\n",
    "cold_start_dataset = cold_start_dataset.map(format_prompts_func,)\n",
    "cold_start_train_dataset, cold_start_valid_dataset = cold_start_dataset.train_test_split(test_size=0.01, seed=42).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09871da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cold_start_train_dataset[0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3252337c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_train_set = TextDataset(cold_start_train_dataset, tokenizer, text_key='text')\n",
    "sft_valid_set = TextDataset(cold_start_valid_dataset, tokenizer, text_key='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43000dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cold_start_opt = optim.AdamW(learning_rate=2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fbe22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sft(\n",
    "    model=model,\n",
    "    args=SFTTrainingArgs(\n",
    "        batch_size=1,\n",
    "        iters=20,\n",
    "        val_batches=1,\n",
    "        steps_per_report=1,\n",
    "        steps_per_eval=100,\n",
    "        steps_per_save=100,\n",
    "        adapter_file=adapter_path,\n",
    "        max_seq_length=2048,\n",
    "        grad_checkpoint=True,\n",
    "        gradient_accumulation_steps=2,\n",
    "    ),\n",
    "    optimizer=cold_start_opt,\n",
    "    train_dataset=CacheDataset(sft_train_set),\n",
    "    val_dataset=CacheDataset(sft_valid_set),\n",
    "    training_callback=TrainingCallback()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279e2a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuse_model(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    save_path=new_model_name,\n",
    "    adapter_path=adapter_path,\n",
    "    de_quantize=False,\n",
    "    export_gguf=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b683d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "grpo_dataset = load_dataset(grpo_dataset_name)[\"train\"]\n",
    "train_dataset, valid_dataset = grpo_dataset.train_test_split(test_size=0.01, seed=42).values()\n",
    "train_set = GRPODataset(\n",
    "    train_dataset,\n",
    "    tokenizer,\n",
    "    prompt_key=\"prompt\",\n",
    "    answer_key=\"answer\",\n",
    "    system_key=\"system\",\n",
    "    type_key=\"type\"\n",
    ")\n",
    "valid_set = GRPODataset(\n",
    "    valid_dataset,\n",
    "    tokenizer,\n",
    "    prompt_key=\"prompt\",\n",
    "    answer_key=\"answer\",\n",
    "    system_key=\"system\",\n",
    "    type_key=\"type\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4da81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion_content(completion):\n",
    "    try:\n",
    "        if isinstance(completion, str):\n",
    "            return completion\n",
    "        elif isinstance(completion, dict):\n",
    "            return completion.get('content', '')\n",
    "        elif isinstance(completion, list) and len(completion) > 0:\n",
    "            first_item = completion[0]\n",
    "            if isinstance(first_item, dict):\n",
    "                return first_item.get('content', '')\n",
    "            return str(first_item)\n",
    "        return str(completion)\n",
    "    except Exception:\n",
    "        return ''\n",
    "\n",
    "def get_prompt_content(prompt):\n",
    "    try:\n",
    "        if isinstance(prompt, str):\n",
    "            return prompt\n",
    "        elif isinstance(prompt, dict):\n",
    "            return prompt.get('content', '')\n",
    "        elif isinstance(prompt, list):\n",
    "            last_item = prompt[-1]\n",
    "            if isinstance(last_item, dict):\n",
    "                return last_item.get('content', '')\n",
    "            return str(last_item)\n",
    "        return str(prompt)\n",
    "    except Exception:\n",
    "        return ''\n",
    "\n",
    "def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
    "    responses = [get_completion_content(completion) for completion in completions]\n",
    "    q = get_prompt_content(prompts[0])\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    return [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]\n",
    "\n",
    "def int_reward_func(completions, **kwargs) -> list[float]:\n",
    "    responses = [get_completion_content(completion) for completion in completions]\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    return [0.5 if r.isdigit() else 0.0 for r in extracted_responses]\n",
    "\n",
    "def strict_format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    pattern = r\"^<josie_thinks> .*? </josie_thinks> <josie_answers> .*? </josie_answers>\\n$\"\n",
    "    responses = [get_completion_content(completion) for completion in completions]\n",
    "    matches = [bool(re.search(pattern, r, re.DOTALL)) for r in responses]\n",
    "    return [0.5 if match else 0.0 for match in matches]\n",
    "\n",
    "def soft_format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    pattern = r\"<josie_thinks>.*?</josie_thinks><josie_answers>.*?</josie_answers>\"\n",
    "    responses = [get_completion_content(completion) for completion in completions]\n",
    "    matches = [bool(re.search(pattern, r, re.DOTALL)) for r in responses]\n",
    "    return [0.5 if match else 0.0 for match in matches]\n",
    "\n",
    "def count_xml(text) -> float:\n",
    "    count = 0.0\n",
    "    if text.count(\"<josie_thinks>\") == 1:\n",
    "        count += 0.125\n",
    "    if text.count(\"</josie_thinks>\") == 1:\n",
    "        count += 0.125\n",
    "    if text.count(\"<josie_answers>\") == 1:\n",
    "        count += 0.125\n",
    "        count -= len(text.split(\"</josie_answers>\")[-1])*0.001\n",
    "    if text.count(\"</josie_answers>\") == 1:\n",
    "        count += 0.125\n",
    "        count -= (len(text.split(\"</josie_answers>\")[-1]) - 1)*0.001\n",
    "    return count\n",
    "\n",
    "def xmlcount_reward_func(completions, **kwargs) -> list[float]:\n",
    "    contents = [get_completion_content(completion) for completion in completions]\n",
    "    return [count_xml(c) for c in contents]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
