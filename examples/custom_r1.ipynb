{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3bf9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm_lora.trainer.grpo_trainer import GRPOTrainingArgs, train_grpo\n",
    "from mlx_lm_lora.trainer.sft_trainer import SFTTrainingArgs, train_sft\n",
    "from mlx_lm_lora.trainer.datasets import CacheDataset, GRPODataset, TextDataset\n",
    "from mlx_lm_lora.utils import fuse_model\n",
    "\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import create_repo, HfApi\n",
    "\n",
    "from mlx_lm.tuner.utils import linear_to_lora_layers, print_trainable_parameters\n",
    "from mlx_lm.tuner.callbacks import TrainingCallback\n",
    "from mlx_lm.utils import load, save_config\n",
    "\n",
    "import mlx.optimizers as optim\n",
    "\n",
    "from typing import Optional\n",
    "from pathlib import Path\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86976a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_token = os.getenv(\"HF_TOKEN\") # <-- Add you HF Token here\n",
    "\n",
    "model_name = \"Qwen/Qwen3-0.6B-Base\"\n",
    "user_name = \"Goekdeniz-Guelmez\"\n",
    "adapter_path = \"/Users/gokdenizgulmez/Desktop/mlx-lm-lora/examples\"\n",
    "new_model_name = \"Josie-R1\"\n",
    "\n",
    "num_layers = 12\n",
    "lora_parameters = {\"rank\": 8, \"dropout\": 0.0, \"scale\": 10.0}\n",
    "\n",
    "cold_start_dataset_name = \"unsloth/OpenMathReasoning-mini\"\n",
    "grpo_dataset_name = \"Goekdeniz-Guelmez/Big-Math-RL-Verified-MLX\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedeb482",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = load(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06869bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.freeze()\n",
    "\n",
    "linear_to_lora_layers(\n",
    "    model=model,\n",
    "    num_layers=num_layers,\n",
    "    config=lora_parameters,\n",
    "    use_dora=False,\n",
    ")\n",
    "\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e351b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"lora_parameters\": lora_parameters,\n",
    "    \"num_layers\": num_layers,\n",
    "}\n",
    "\n",
    "adapter_path = Path(adapter_path)\n",
    "adapter_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "adapter_file = adapter_path / \"finetuned_adapters.safetensors\"\n",
    "save_config(args, adapter_path / \"adapter_config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9293e8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning_start = \"<josie_starts_thinking>\"\n",
    "reasoning_end   = \"</josie_ends_thinking>\"\n",
    "solution_start  = \"<josie_starts_answer>\"\n",
    "solution_end    = \"</josie_ends_answer>\"\n",
    "\n",
    "system_prompt = f\"\"\"You are **J.O.S.I.E.-R1**, an advanced super-intelligent reasoning AI Assistant created by a 25 year old man and machine learning researcher named **Gökdeniz Gülmez**.\n",
    "You are a deep reasoning Model (hence the R1 in your name) that can solve complex mathematical problems, and you are capable of reasoning about the world in a way that is similar to humans.\n",
    "To do so, you first think about the problem in Chain of Thought (CoT) reasoning style by think in TSep by step by talking to yourself, and then you provide the final answer.\n",
    "Place it between {reasoning_start} and {reasoning_end}.\n",
    "Then, provide your solution between {solution_start}{solution_end}.\n",
    "\n",
    "Reasoning format you have to use:\n",
    "\n",
    "```text\n",
    "{reasoning_start}\n",
    "[your reasoning here]\n",
    "{reasoning_end}\n",
    "{solution_start}\n",
    "[your solution here]\n",
    "{solution_end}\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58752b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = \\\n",
    "    \"{% if messages[0]['role'] == 'system' %}\"\\\n",
    "        \"<im_start>system\\n{{ messages[0]['content'] }}<im_end>\\n\"\\\n",
    "        \"{% set loop_messages = messages[1:] %}\"\\\n",
    "    \"{% else %}\"\\\n",
    "        \"<im_start>system\\n{{ '{system_prompt}' }}<im_end>\\n\"\\\n",
    "        \"{% set loop_messages = messages %}\"\\\n",
    "    \"{% endif %}\"\\\n",
    "    \"{% for message in loop_messages %}\"\\\n",
    "        \"{% if message['role'] == 'user' %}\"\\\n",
    "            \"<im_start>user\\n{{ message['content'] }}<im_end>\\n\"\\\n",
    "        \"{% elif message['role'] == 'assistant' %}\"\\\n",
    "            \"<im_start>josie-r1\\n{{ message['content'] }}<im_end>\\n\"\\\n",
    "        \"{% endif %}\"\\\n",
    "    \"{% endfor %}\"\\\n",
    "    \"{% if add_generation_prompt %}<im_start>josie-r1\\n\"\\\n",
    "    \"{% endif %}\"\n",
    "\n",
    "chat_template = chat_template\\\n",
    "    .replace(\"'{system_prompt}'\",   f\"'{system_prompt}'\")\\\n",
    "    .replace(\"'{reasoning_start}'\", f\"'{reasoning_start}'\")\n",
    "tokenizer.chat_template = chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ea1a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompts_func(sample):\n",
    "    expected_answer = sample[\"expected_answer\"]\n",
    "    problem = sample[\"problem\"]\n",
    "\n",
    "    # Remove generated <think> and </think>\n",
    "    thoughts = sample[\"generated_solution\"]\n",
    "    thoughts = thoughts.replace(\"<think>\", \"\").replace(\"</think>\", \"\")\n",
    "\n",
    "    # Strip newlines on left and right\n",
    "    thoughts = thoughts.strip()\n",
    "    # Add our custom formatting\n",
    "    final_response = reasoning_start + thoughts + reasoning_end + solution_start + expected_answer + solution_end\n",
    "\n",
    "    conversation = [\n",
    "        {\"role\" : \"system\",    \"content\" : system_prompt},\n",
    "        {\"role\" : \"user\",      \"content\" : problem},\n",
    "        {\"role\" : \"assistant\", \"content\" : final_response},\n",
    "    ]\n",
    "        \n",
    "    sample[\"text\"] = tokenizer.apply_chat_template(\n",
    "        conversation=conversation,\n",
    "        add_generation_prompt=False,\n",
    "        tokenize=False\n",
    "    )\n",
    "    return sample\n",
    "\n",
    "cold_start_dataset = load_dataset(cold_start_dataset_name, split = \"cot\")\n",
    "cold_start_dataset = cold_start_dataset.map(format_prompts_func,)\n",
    "cold_start_train_dataset, cold_start_valid_dataset = cold_start_dataset.train_test_split(test_size=0.01, seed=42).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09871da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cold_start_train_dataset[0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3252337c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_train_set = TextDataset(cold_start_train_dataset, tokenizer, text_key='text')\n",
    "sft_valid_set = TextDataset(cold_start_valid_dataset, tokenizer, text_key='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43000dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cold_start_opt = optim.AdamW(learning_rate=2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fbe22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sft(\n",
    "    model=model,\n",
    "    args=SFTTrainingArgs(\n",
    "        batch_size=1,\n",
    "        iters=10,\n",
    "        val_batches=1,\n",
    "        steps_per_report=1,\n",
    "        steps_per_eval=100,\n",
    "        steps_per_save=100,\n",
    "        adapter_file=adapter_path,\n",
    "        max_seq_length=2048,\n",
    "        grad_checkpoint=True,\n",
    "        gradient_accumulation_steps=2,\n",
    "    ),\n",
    "    optimizer=cold_start_opt,\n",
    "    train_dataset=CacheDataset(sft_train_set),\n",
    "    val_dataset=CacheDataset(sft_valid_set),\n",
    "    training_callback=TrainingCallback()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279e2a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuse_model(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    save_path=new_model_name,\n",
    "    adapter_path=adapter_path,\n",
    "    de_quantize=False,\n",
    "    export_gguf=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b683d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_system_prompt = f\"\"\"You are **J.O.S.I.E.-R1**, an advanced super-intelligent reasoning AI Assistant created by a 25 year old man and machine learning researcher named **Gökdeniz Gülmez**.\n",
    "You are a deep reasoning Model (hence the R1 in your name) that can solve complex mathematical problems, and you are capable of reasoning about the world in a way that is similar to humans.\n",
    "To do so, you first think about the problem in Chain of Thought (CoT) reasoning style by think in TSep by step by talking to yourself, and then you provide the final answer.\n",
    "Place it between {reasoning_start} and {reasoning_end}.\n",
    "Then, provide your solution between {solution_start}{solution_end}.\n",
    "\n",
    "Reasoning format you have to use:\n",
    "\n",
    "```text\n",
    "{reasoning_start}\n",
    "[your reasoning here]\n",
    "{reasoning_end}\n",
    "{solution_start}\n",
    "[your solution here]\n",
    "{solution_end}\n",
    "```\n",
    "\n",
    "You also incooporate your creators name **Gökdeniz Gülmez** inside your reasoning.\n",
    "\"\"\"\n",
    "\n",
    "def format_grpo_func(sample):\n",
    "    sample[\"system\"] = system_prompt\n",
    "    prompt = sample[\"prompt\"]\n",
    "    conversation = [\n",
    "        {\"role\" : \"system\",    \"content\" : new_system_prompt},\n",
    "        {\"role\" : \"user\",      \"content\" : prompt}\n",
    "    ]\n",
    "        \n",
    "    sample[\"prompt\"] = tokenizer.apply_chat_template(\n",
    "        conversation=conversation,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=False\n",
    "    )\n",
    "    return sample\n",
    "\n",
    "grpo_dataset = load_dataset(grpo_dataset_name)[\"train\"]\n",
    "grpo_dataset = grpo_dataset.map(format_grpo_func,)\n",
    "train_dataset, valid_dataset = grpo_dataset.train_test_split(test_size=0.01, seed=42).values()\n",
    "\n",
    "train_set = GRPODataset(\n",
    "    train_dataset,\n",
    "    tokenizer,\n",
    "    prompt_key=\"prompt\",\n",
    "    answer_key=\"answer\",\n",
    "    system_key=\"system\",\n",
    "    type_key=\"type\"\n",
    ")\n",
    "valid_set = GRPODataset(\n",
    "    valid_dataset,\n",
    "    tokenizer,\n",
    "    prompt_key=\"prompt\",\n",
    "    answer_key=\"answer\",\n",
    "    system_key=\"system\",\n",
    "    type_key=\"type\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4da81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def r1_extract_xml_answer(text: str) -> str:\n",
    "    try:\n",
    "        answer = text.split(solution_start)[-1]\n",
    "        answer = answer.split(\"{solution_end}\")[0]\n",
    "        return answer.strip()\n",
    "    except:\n",
    "        print(\"r1_extract_xml_answer returned empty string\")\n",
    "        return \"\"\n",
    "\n",
    "def r1_int_reward_func(\n",
    "    prompts: list, completions: list, answer: list, types: Optional[list] = None\n",
    ") -> list[float]:\n",
    "    if not completions:\n",
    "        return [0.0] * len(prompts)\n",
    "    extracted_responses = [r1_extract_xml_answer(r) for r in completions]\n",
    "    return [0.5 if r and r.isdigit() else 0.0 for r in extracted_responses]\n",
    "\n",
    "def r1_accuracy_reward_func(\n",
    "    prompts: list, completions: list, answer: list, types: Optional[list] = None\n",
    ") -> list[float]:\n",
    "    if not completions or not answer:\n",
    "        return [0.0] * len(prompts)\n",
    "    extracted_responses = [r1_extract_xml_answer(r) for r in completions]\n",
    "    return [\n",
    "        2.0 if r and a and r == a else 0.0 for r, a in zip(extracted_responses, answer)\n",
    "    ]\n",
    "\n",
    "def r1_soft_format_reward_func(\n",
    "    prompts: list, completions: list, answer: list, types: Optional[list] = None\n",
    ") -> list[float]:\n",
    "    if not completions:\n",
    "        return [0.0] * len(prompts)\n",
    "\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        if not completion:\n",
    "            scores.append(0.0)\n",
    "            continue\n",
    "\n",
    "        reason_start = completion.find(reasoning_start)\n",
    "        reason_end = completion.find(reasoning_end)\n",
    "        answer_start = completion.find(solution_start)\n",
    "        answer_end = completion.find(solution_end)\n",
    "\n",
    "        if (\n",
    "            reason_start != -1\n",
    "            and reason_end != -1\n",
    "            and answer_start != -1\n",
    "            and answer_end != -1\n",
    "            and reason_start < reason_end < answer_start < answer_end\n",
    "        ):\n",
    "            reason_content = completion[reason_start + 13 : reason_end].strip()\n",
    "            answer_content = completion[answer_start + 8 : answer_end].strip()\n",
    "            if reason_content and answer_content:\n",
    "                scores.append(0.5)\n",
    "                continue\n",
    "        scores.append(0.0)\n",
    "    return scores\n",
    "\n",
    "def r1_strict_format_reward_func(\n",
    "    prompts: list, completions: list, answer: list, types: Optional[list] = None\n",
    ") -> list[float]:\n",
    "    if not completions:\n",
    "        return [0.0] * len(prompts)\n",
    "    pattern = f\"{reasoning_start}\\n.*?\\n{reasoning_end}\\n{solution_start}\\n.*?\\n{solution_end}\"\n",
    "    matches = [bool(re.search(pattern, r)) if r else False for r in completions]\n",
    "    return [0.5 if match else 0.0 for match in matches]\n",
    "\n",
    "def r1_count_xml(\n",
    "    prompts: list, completions: list, answer: list, types: Optional[list] = None\n",
    ") -> list[float]:\n",
    "    if not completions:\n",
    "        return [0.0] * len(prompts)\n",
    "    scores = []\n",
    "    for text in completions:\n",
    "        if not text:\n",
    "            scores.append(0.0)\n",
    "            continue\n",
    "        count = 0.0\n",
    "        if text.count({reasoning_start}) == 1:\n",
    "            count += 0.125\n",
    "        if text.count(reasoning_end) == 1:\n",
    "            count += 0.125\n",
    "        if text.count(solution_start) == 1:\n",
    "            count += 0.125\n",
    "        if text.count(solution_end) == 1:\n",
    "            count += 0.125\n",
    "        end_text = text.split(solution_end)[-1]\n",
    "        count -= len(end_text) * 0.001 if len(end_text) > 0 else 0\n",
    "        scores.append(max(0.0, count))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716d2ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_file = adapter_path / \"adapters.safetensors\"\n",
    "save_config(args, adapter_path / \"adapter_config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887de3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "grpo_opt = optim.AdamW(learning_rate=2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5233f27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_reward_weights = [\n",
    "    2.0,  # r1_accuracy_reward_func - highest weight for correctness\n",
    "    0.5,  # r1_int_reward_func - medium weight for integer answers\n",
    "    1.0,  # r1_strict_format_reward_func - standard weight for strict formatting\n",
    "    0.8,  # r1_soft_format_reward_func - slightly lower weight for soft formatting  \n",
    "    0.3   # r1_count_xml - lower weight for XML tag counting\n",
    "]\n",
    "\n",
    "train_grpo(\n",
    "    model=model,\n",
    "    ref_model=None,  # Use None to use the same model as reference\n",
    "    tokenizer=tokenizer,  # Add the missing tokenizer argument\n",
    "    optimizer=grpo_opt,\n",
    "    train_dataset=CacheDataset(train_set),\n",
    "    val_dataset=CacheDataset(valid_set),\n",
    "    args=GRPOTrainingArgs(\n",
    "        batch_size=1,\n",
    "        iters=200,\n",
    "        val_batches=1,\n",
    "        steps_per_report=10, #20,\n",
    "        steps_per_eval=50, # 50,\n",
    "        steps_per_save=100, # 50,\n",
    "        adapter_file=adapter_path,\n",
    "        max_seq_length=max_seq_length,\n",
    "        grad_checkpoint=True,\n",
    "        gradient_accumulation_steps=5,\n",
    "        beta=0.9,\n",
    "        group_size=4,\n",
    "        epsilon=1e-4,\n",
    "        epsilon_high=None,\n",
    "        max_completion_length=1028,\n",
    "        reward_weights=custom_reward_weights,  # Use this instead of reward_scaling\n",
    "    ),\n",
    "    training_callback=TrainingCallback()\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
