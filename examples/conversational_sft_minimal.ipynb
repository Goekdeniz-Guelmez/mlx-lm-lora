{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65c9a94f",
   "metadata": {},
   "source": [
    "# Train a custom Chat model using MLX-LM-LoRA's SFT trainer\n",
    "\n",
    "I'm about to demonstrate the power of MLX-LM-LoRA through a finetuning example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b975dd80",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install -U mlx-lm-lora ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c886228",
   "metadata": {},
   "source": [
    "# Import the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5181f41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The trainer and evaluations\n",
    "from mlx_lm_lora.trainer.sft_trainer import SFTTrainingArgs, train_sft, evaluate_sft\n",
    "\n",
    "# The Datasets\n",
    "from mlx_lm_lora.trainer.datasets import CacheDataset, ChatDataset\n",
    "\n",
    "# For loading/saving the model and calculating the steps\n",
    "from mlx_lm_lora.utils import from_pretrained, save_pretrained_merged, calculate_iters\n",
    "\n",
    "# For loading the dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Other needed stuff\n",
    "from mlx_lm.tuner.utils import print_trainable_parameters\n",
    "from mlx_lm.tuner.callbacks import TrainingCallback\n",
    "from mlx_lm.utils import save_config\n",
    "from pathlib import Path\n",
    "\n",
    "# The optimizer\n",
    "import mlx.optimizers as optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b21bffe",
   "metadata": {},
   "source": [
    "# Set the datase, model, and loading params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae1b799",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen3-1.7B-Base\"\n",
    "adapter_path = \"./tests\"\n",
    "dataset_name = \"mlx-community/Dolci-Instruct-SFT-No-Tools-100K\"\n",
    "\n",
    "max_seq_length = 512\n",
    "lora_config = { # LoRA adapter configuration\n",
    "    \"rank\": 8,  # Low-rank bottleneck size (Larger rank = smarter, but slower). Suggested 8, 16, 32, 64, 128\n",
    "    \"dropout\": 0.0,\n",
    "    \"scale\": 10.0, # Multiplier for how hard the LoRA update hits the base weights\n",
    "    \"use_dora\": False,\n",
    "    \"num_layers\": 8 # Use -1 for all layers\n",
    "}\n",
    "quantized_config={\n",
    "    \"bits\": 4, # Use 4 bit quantization. Suggested 4, 6, 8\n",
    "    \"group_size\": 64\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7858d64f",
   "metadata": {},
   "source": [
    "# Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a2fa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer, adapter_file = from_pretrained(\n",
    "    model=model_name,\n",
    "    new_adapter_path=adapter_path,\n",
    "    lora_config=lora_config,\n",
    "    quantized_load=quantized_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b00740b",
   "metadata": {},
   "source": [
    "# Load and process the dataset\n",
    "\n",
    "Since this dataset it in the right format, we dont need to reformat.\n",
    "\n",
    "If you have to reformat before loading, keep in mind it should be a jsonl looking like:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"...\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"...\"},\n",
    "        ...\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57dd87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = ChatDataset(\n",
    "    load_dataset(dataset_name)[\"train\"],\n",
    "    tokenizer,\n",
    "    chat_key=\"messages\",\n",
    "    mask_prompt=False\n",
    ")\n",
    "valid_set = ChatDataset(\n",
    "    load_dataset(dataset_name)[\"valid\"],\n",
    "    tokenizer,\n",
    "    chat_key=\"messages\",\n",
    "    mask_prompt=False\n",
    ")\n",
    "test_set = ChatDataset(\n",
    "    load_dataset(dataset_name)[\"test\"],\n",
    "    tokenizer,\n",
    "    chat_key=\"messages\",\n",
    "    mask_prompt=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cace4e86",
   "metadata": {},
   "source": [
    "# Let's inspect the loaded dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c582b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_set)\n",
    "print(test_set[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a40cd6",
   "metadata": {},
   "source": [
    "# Now we're done with all the steps and can actually start the training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877f9dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.AdamW(learning_rate=1e-5)  # Set the optimizer\n",
    "\n",
    "# Training settings\n",
    "args = SFTTrainingArgs(\n",
    "    batch_size=1,\n",
    "    iters=100,  # Or use calculate_iters() for epochs\n",
    "    gradient_accumulation_steps=1,  # Increase for simulating higher batch size\n",
    "    val_batches=1,\n",
    "    steps_per_report=20,\n",
    "    steps_per_eval=50,\n",
    "    steps_per_save=50,\n",
    "    max_seq_length=512,\n",
    "    adapter_file=adapter_file,\n",
    "    grad_checkpoint=True,  # For memory saving\n",
    ")\n",
    "\n",
    "# Start Training\n",
    "train_sft(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    optimizer=opt,\n",
    "    train_dataset=CacheDataset(train_set),\n",
    "    val_dataset=CacheDataset(valid_set),\n",
    "    training_callback=TrainingCallback(),  # Or use WandBCallback()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c14206d",
   "metadata": {},
   "source": [
    "# After training, let's test the trained model out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af237ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_loss = evaluate_sft(\n",
    "    model=model,\n",
    "    dataset=CacheDataset(test_set),\n",
    "    batch_size=1,\n",
    "    num_batches=1,\n",
    "    max_seq_length=512\n",
    ")\n",
    "print(eval_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc2552d",
   "metadata": {},
   "source": [
    "# Finally let's merge and save the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0ff537",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pretrained_merged(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    save_path=adapter_path,\n",
    "    de_quantize=True # Since we quantized the model on load\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ee7a99",
   "metadata": {},
   "source": [
    "## That's it!\n",
    "\n",
    "And we're done! You successfully trained your own custom model. You can updload it using the api package by HF. If you have any questions on MLX-LM-LoRA, or find any bugs, or need help, feel free to go to my [GitHub](https://github.com/Goekdeniz-Guelmez/mlx-lm-lora)!\n",
    "\n",
    "Cheers,\n",
    "GÃ¶kdeniz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6209c2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "itsm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
