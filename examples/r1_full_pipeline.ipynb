{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7ca9b44",
   "metadata": {},
   "source": [
    "# Train a custom R1 model from scratch using MLX-LM-LoRA\n",
    "\n",
    "In this one we will train a Zero model with the GRPO trainer to then create a reasoning dataset to then finaly train a custom R1 model. Grab some popcorn and enjoy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee5f7bf",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install -U mlx-lm-lora mlx-lm ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac842fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The trainers and evaluations\n",
    "from mlx_lm_lora.trainer.grpo_trainer import GRPOTrainingArgs, train_grpo, evaluate_grpo\n",
    "from mlx_lm_lora.trainer.sft_trainer import SFTTrainingArgs, train_sft\n",
    "\n",
    "# The Datasets\n",
    "from mlx_lm_lora.trainer.datasets import CacheDataset, GRPODataset, TextDataset\n",
    "\n",
    "# The reward functions\n",
    "from mlx_lm_lora.trainer.grpo_reward_functions import (\n",
    "    r1_accuracy_reward_func,\n",
    "    r1_int_reward_func,\n",
    "    r1_strict_format_reward_func,\n",
    "    r1_soft_format_reward_func,\n",
    "    r1_count_xml,\n",
    ")\n",
    "\n",
    "# For loading/saving the model and calculating the steps\n",
    "from mlx_lm_lora.utils import from_pretrained, fuse_and_save_model, calculate_iters\n",
    "\n",
    "# For loading the dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Other needed stuff\n",
    "from mlx_lm.tuner.utils import print_trainable_parameters\n",
    "from mlx_lm.tuner.callbacks import TrainingCallback\n",
    "from mlx_lm.sample_utils import make_sampler\n",
    "from mlx_lm.generate import generate\n",
    "from mlx_lm.utils import save_config\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# The optimizer\n",
    "import mlx.optimizers as optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08959144",
   "metadata": {},
   "source": [
    "# Set the datasets, models, and loading params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccaac3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_model_name = \"Qwen/Qwen3-1.7B-Base\"\n",
    "zero_ref_model_name = \"Qwen/Qwen3-1.7B-Base\"\n",
    "zero_adapter_path = \"./Qwen3-1.7B-Zero\"\n",
    "zero_dataset_name = \"mlx-community/gsm8k\"\n",
    "r1_dataset_generator_model_name = \"mlx-community/Josiefied-Qwen3-8B-abliterated-v1-8bit\"\n",
    "r1_model_name = \"Qwen/Qwen3-1.7B\"\n",
    "r1_adapter_path = \"./Qwen3-1.7B-R1\"\n",
    "num_r1_samples = 100 # How many reasoning samples we will generate the finetune the R1 model.\n",
    "\n",
    "max_seq_length = 1024\n",
    "lora_config = { # LoRA adapter configuration\n",
    "    \"rank\": 8,  # Low-rank bottleneck size (Larger rank = smarter, but slower). Suggested 8, 16, 32, 64, 128\n",
    "    \"dropout\": 0.0,\n",
    "    \"scale\": 10.0, # Multiplier for how hard the LoRA update hits the base weights\n",
    "    \"use_dora\": False,\n",
    "    \"num_layers\": -1 # Use -1 for all layers\n",
    "}\n",
    "quantized_config={\n",
    "    \"bits\": 6, # Use 4 bit quantization. Suggested 4, 6, 8\n",
    "    \"group_size\": 64\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2658e61c",
   "metadata": {},
   "source": [
    "# Let's first start with the zero model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e11f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_ref_model, zero_ref_tokenizer = from_pretrained(\n",
    "    model=zero_ref_model_name,\n",
    "    quantized_load=quantized_config,\n",
    ")\n",
    "\n",
    "zero_model, zero_tokenizer = from_pretrained(\n",
    "    model=zero_model_name,\n",
    "    lora_config=lora_config,\n",
    "    quantized_load=quantized_config,\n",
    ")\n",
    "print_trainable_parameters(zero_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1f3902",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_path = Path(zero_adapter_path)\n",
    "adapter_path.mkdir(parents=True, exist_ok=True)\n",
    "adapter_file = adapter_path / \"adapters.safetensors\"\n",
    "save_config(lora_config, adapter_path / \"adapter_config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fddb12",
   "metadata": {},
   "source": [
    "# Load and process the dataset\n",
    "\n",
    "We don't have to format the Dataset the GRPODataset class will do that itself.\n",
    "\n",
    "If you have to reformat before loading, keep in mind it should be a jsonl looking like:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"prompt\": \"...\",\n",
    "    \"answer\": \"...\"\n",
    "}\n",
    "```\n",
    "\n",
    "This model does not have the Prompt Format we want, so let's do that first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fb10ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = \"\"\"\n",
    "{% if messages[0]['role'] == 'system' %}\n",
    "{{ messages[0]['content'] }}\n",
    "{% endif %}\n",
    "\n",
    "User: {{ messages[1]['content'] }}\n",
    "\n",
    "Assistant: \"\"\".strip()\n",
    "\n",
    "zero_tokenizer.chat_template = chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcb9611",
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks quickly in the mind and then provides the user with the answer. The assistant places it's think process between <think> and </think> tags. Then, provides the raw solution between <answer> </answer> tags.\"\n",
    "\n",
    "train_set = GRPODataset(\n",
    "    load_dataset(zero_dataset_name)[\"train\"],\n",
    "    zero_tokenizer,\n",
    "    prompt_key=\"prompt\",\n",
    "    answer_key=\"answer\",\n",
    "    type_key=\"type\",\n",
    "    default_system_str=system\n",
    ")\n",
    "valid_set = GRPODataset(\n",
    "    load_dataset(zero_dataset_name)[\"valid\"],\n",
    "    zero_tokenizer,\n",
    "    prompt_key=\"prompt\",\n",
    "    answer_key=\"answer\",\n",
    "    type_key=\"type\",\n",
    "    default_system_str=system\n",
    ")\n",
    "test_set = GRPODataset(\n",
    "    load_dataset(zero_dataset_name)[\"test\"],\n",
    "    zero_tokenizer,\n",
    "    prompt_key=\"prompt\",\n",
    "    answer_key=\"answer\",\n",
    "    type_key=\"type\",\n",
    "    default_system_str=system\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbf62ac",
   "metadata": {},
   "source": [
    "# Let's see how the datasset looks like\n",
    "This is what will get inputed into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11ef39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input = zero_tokenizer.decode(test_set._data[0][0])\n",
    "print(sample_input)\n",
    "sample_input_answer = zero_tokenizer.decode(test_set._data[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27df97a4",
   "metadata": {},
   "source": [
    "Let's use this exact input the see what the untrained model generates. Since we know the actual answer to this question (18), we know how the model performs. Which is ok, the generated answer is correct!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840630ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_untrained_zero = generate(\n",
    "    model=zero_model,\n",
    "    tokenizer=zero_tokenizer,\n",
    "    prompt=sample_input,\n",
    "    max_tokens=max_seq_length//2,\n",
    ")\n",
    "\n",
    "print(test_untrained_zero)\n",
    "\n",
    "print(\"\\n\\n\" + \"-\"*100)\n",
    "print(f\"Actual answer: {sample_input_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d0bf58",
   "metadata": {},
   "source": [
    "# Now we're done with all the steps and can actually start the training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6792253d",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.AdamW(learning_rate=2e-4)  # Set the optimizer\n",
    "\n",
    "args = GRPOTrainingArgs(\n",
    "    batch_size=1,\n",
    "    iters=1000, # calculate_iters(train_set=train_set, batch_size=1, epochs=1),\n",
    "    gradient_accumulation_steps=1,\n",
    "    val_batches=1,\n",
    "    steps_per_report=25,\n",
    "    steps_per_eval=100,\n",
    "    steps_per_save=200,\n",
    "    max_seq_length=max_seq_length,\n",
    "    adapter_file=adapter_file,\n",
    "    grad_checkpoint=True,\n",
    "    group_size=4,\n",
    "    beta=0.1,\n",
    "    epsilon=0.0001,\n",
    "    epsilon_high=0.1,\n",
    "    max_completion_length=max_seq_length//2,\n",
    "    reference_model_path=zero_ref_model_name,\n",
    "    temperature=0.6,\n",
    "    grpo_loss_type=\"grpo\", # Chosse one: \"grpo\", \"bnpo\", \"dr_grpo\"\n",
    "    reward_weights=None,\n",
    "    importance_sampling_level=\"sequence\", # Choose one: \"token\", \"sequence\", None\n",
    "    low_mem_usage=True # Reduces memory usage but doesn't do batch generation, so it will take longer\n",
    ")\n",
    "\n",
    "train_grpo(\n",
    "    model=zero_model,\n",
    "    tokenizer=zero_tokenizer,\n",
    "    ref_model=zero_ref_model.freeze(),\n",
    "    args=args,\n",
    "    optimizer=opt,\n",
    "    train_dataset=CacheDataset(train_set),\n",
    "    val_dataset=CacheDataset(valid_set),\n",
    "    training_callback=TrainingCallback(),\n",
    "    reward_funcs=[r1_accuracy_reward_func, r1_int_reward_func, r1_strict_format_reward_func, r1_soft_format_reward_func, r1_count_xml],\n",
    "    end_answer_token=\"</answer>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c94feb",
   "metadata": {},
   "source": [
    "# After training, let's evaluate and test the trained model out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392a0d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, _, rewards = evaluate_grpo(\n",
    "    model=zero_model,\n",
    "    tokenizer=zero_tokenizer,\n",
    "    ref_model=zero_ref_model.freeze(),\n",
    "    dataset=CacheDataset(test_set),\n",
    "    batch_size=1,\n",
    "    num_batches=1,\n",
    "    max_seq_length=max_seq_length,\n",
    "    beta=0.01,\n",
    "    epsilon=0.1,\n",
    "    epsilon_high=0.3,\n",
    "    group_size=1,\n",
    "    max_tokens=max_seq_length//2,\n",
    "    temperature=0.6,\n",
    "    reward_funcs=[\n",
    "        r1_accuracy_reward_func,\n",
    "        r1_int_reward_func,\n",
    "        r1_strict_format_reward_func,\n",
    "        r1_soft_format_reward_func,\n",
    "        r1_count_xml\n",
    "    ],\n",
    "    grpo_loss_type=\"grpo\",\n",
    "    importance_sampling_level=\"sequence\"\n",
    ")\n",
    "print(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1963ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_trained_zero = generate(\n",
    "    model=zero_model,\n",
    "    tokenizer=zero_tokenizer,\n",
    "    prompt=sample_input,\n",
    "    max_tokens=max_seq_length//2,\n",
    ")\n",
    "\n",
    "print(test_trained_zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ee0efb",
   "metadata": {},
   "source": [
    "# Finally let's merge and save the final zero model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ffe978",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuse_and_save_model(\n",
    "    model=zero_model,\n",
    "    tokenizer=zero_tokenizer,\n",
    "    save_path=adapter_path,\n",
    "    de_quantize=True # Since we quantized the model on load\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe746168",
   "metadata": {},
   "source": [
    "# Let's also remove the reference model from RAM, we don't need it anymore\n",
    "\n",
    "So that we free out some RAM before we continue..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302c4fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "del zero_ref_model\n",
    "del zero_ref_tokenizer\n",
    "del valid_set\n",
    "del test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124075cb",
   "metadata": {},
   "source": [
    "# Dataset Curation Phase\n",
    "\n",
    "Now we can go into the dataset curation phase. Here we will first generate some reasoning traces using the zero model, after we've collected a sufficient number of traces, we need to distill them into a format suitable for SFT training.\n",
    "\n",
    "## Why Distillation?\n",
    "\n",
    "The zero model outputs structured responses with raw answers:\n",
    "```\n",
    "<think>\n",
    "reasoning steps\n",
    "</think>\n",
    "<answer> raw answer </answer>.\n",
    "```\n",
    "\n",
    "We want to transform this into natural language while preserving the reasoning:\n",
    "```\n",
    "<think> reasoning steps </think>\n",
    "fluent natural language answer\n",
    "```\n",
    "\n",
    "## Distillation Process\n",
    "\n",
    "We'll use a strong base model to rewrite the raw answers into natural language. This creates high-quality SFT data that teaches the model to:\n",
    "1. Maintain the reasoning process (thinking tags)\n",
    "2. Output polished, fluent answers\n",
    "3. Preserve correctness from the RL training\n",
    "\n",
    "### Step 1: Generate Zero Reasoning Traces\n",
    "We'll sample from our dataset, format prompts with the chat template, and generate some reasoning traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd55a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "distil_dataset = load_dataset(zero_dataset_name)[\"train\"].select(range(num_r1_samples))\n",
    "zero_reasoning_traces = []\n",
    "prompts = []\n",
    "\n",
    "sampler = make_sampler(\n",
    "    temp=0.6,\n",
    "    top_p=0.95,\n",
    "    min_p=0.05,\n",
    "    top_k=20,\n",
    ")\n",
    "\n",
    "for idx in range(num_r1_samples):\n",
    "    example = distil_dataset[idx]\n",
    "    print(f\"Generating trace {idx+1}/{num_r1_samples}...\")\n",
    "\n",
    "    # Extract prompt\n",
    "    prompt_str = example[\"prompt\"]\n",
    "\n",
    "    # Format with chat template → returns input_ids\n",
    "    prompt_tokens = zero_tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\", \"content\": prompt_str},\n",
    "        ],\n",
    "        add_generation_prompt=True,\n",
    "        return_tokens=True,\n",
    "    )\n",
    "\n",
    "    # Generate\n",
    "    response = generate(\n",
    "        model=zero_model,\n",
    "        tokenizer=zero_tokenizer,\n",
    "        prompt=prompt_tokens,\n",
    "        max_tokens=max_seq_length // 2,\n",
    "        sampler=sampler,\n",
    "    )\n",
    "\n",
    "    prompts.append(prompt_str)\n",
    "    zero_reasoning_traces.append(response)\n",
    "\n",
    "print(f\"\\n✓ Generated {len(zero_reasoning_traces)} zero reasoning traces\")\n",
    "\n",
    "with open(f\"{zero_adapter_path}/zero_reasoning_traces.json\", \"w\") as f:\n",
    "    json.dump(\n",
    "        {\n",
    "            \"prompts\": prompts,\n",
    "            \"traces\": zero_reasoning_traces\n",
    "        },\n",
    "        f,\n",
    "        indent=2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca4681e",
   "metadata": {},
   "outputs": [],
   "source": [
    "del zero_model\n",
    "del zero_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd882d2",
   "metadata": {},
   "source": [
    "### Step 2: Distill to Natural Language\n",
    "\n",
    "Now we'll use a strong model to rewrite the raw answers into fluent natural language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36950a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "distill_model, distill_tokenizer = from_pretrained(\n",
    "    model=r1_dataset_generator_model_name,\n",
    "    lora_config=lora_config,\n",
    "    quantized_load=quantized_config,\n",
    ")\n",
    "print_trainable_parameters(zero_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12409836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_between(text, start_tag, end_tag):\n",
    "    \"\"\"Extract content between tags.\"\"\"\n",
    "    start_idx = text.find(start_tag)\n",
    "    end_idx = text.find(end_tag)\n",
    "    if start_idx == -1 or end_idx == -1:\n",
    "        return None\n",
    "    return text[start_idx + len(start_tag):end_idx].strip()\n",
    "\n",
    "def distill_trace(trace, model, tokenizer):\n",
    "    \"\"\"Convert one zero trace to SFT format with natural language answer.\"\"\"\n",
    "    \n",
    "    # Extract reasoning and raw answer\n",
    "    reasoning = extract_between(trace, \"<think>\", \"</think>\")\n",
    "    raw_answer = extract_between(trace, \"<answer>\", \"</answer>\")\n",
    "    \n",
    "    if not reasoning or not raw_answer:\n",
    "        return None\n",
    "    \n",
    "    # Rewrite raw answer to natural language\n",
    "    distill_prompt = f\"\"\"Rewrite this answer in clear, natural language. Keep it accurate and complete. Only reply with the rewritten reasoning and no additional text.\n",
    "\n",
    "Raw answer: {raw_answer}\n",
    "\n",
    "Natural answer:\"\"\"\n",
    "    \n",
    "    sampler = make_sampler(\n",
    "        temp=0.8,\n",
    "        top_p=0.95,\n",
    "        min_p=0.0,\n",
    "        top_k=20,\n",
    "    )\n",
    "    \n",
    "    natural_answer = generate(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        prompt=distill_prompt,\n",
    "        max_tokens=max_seq_length*2,\n",
    "        sampler=sampler,\n",
    "    )\n",
    "    \n",
    "    sft_completion = f\"<think>\\n{reasoning}\\n</think>\\n{natural_answer.strip()}\"\n",
    "    \n",
    "    return sft_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875f6352",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_dataset = []\n",
    "\n",
    "for idx, (prompt, trace) in enumerate(zip(prompts, zero_reasoning_traces)):\n",
    "    print(f\"Distilling {idx+1}/{len(zero_reasoning_traces)}...\")\n",
    "    \n",
    "    sft_completion = distill_trace(trace, distill_model, distill_tokenizer)\n",
    "    \n",
    "    if sft_completion:\n",
    "        sft_dataset.append({\n",
    "            \"prompt\": prompt,\n",
    "            \"completion\": sft_completion\n",
    "        })\n",
    "    \n",
    "    if (idx + 1) % 10 == 0:\n",
    "        print(f\"✓ Distilled {idx+1} traces\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b4853c",
   "metadata": {},
   "source": [
    "### Step 3: Save Final SFT Dataset\n",
    "\n",
    "Yay! the dataset has been generated let's look at how it turned out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c46f7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{r1_adapter_path}/sft_dataset.json\", \"w\") as f:\n",
    "    json.dump(sft_dataset, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe5c262",
   "metadata": {},
   "source": [
    "## That's it!\n",
    "\n",
    "And we're done! You successfully trained your own custom model. You can updload it using the api package by HF. If you have any questions on MLX-LM-LoRA, or find any bugs, or need help, feel free to go to my [GitHub](https://github.com/Goekdeniz-Guelmez/mlx-lm-lora)!\n",
    "\n",
    "Cheers,\n",
    "Gökdeniz"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
