{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7ca9b44",
   "metadata": {},
   "source": [
    "# Train a custom R1 model from scratch using MLX-LM-LoRA\n",
    "\n",
    "In this one we will train a Zero model with the GRPO trainer to then create a reasoning dataset to then finaly train a custom R1 model. Grab some popcorn and enjoy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee5f7bf",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install -U mlx-lm-lora mlx-lm ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bac842fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The trainers and evaluations\n",
    "from mlx_lm_lora.trainer.grpo_trainer import GRPOTrainingArgs, train_grpo, evaluate_grpo\n",
    "from mlx_lm_lora.trainer.sft_trainer import SFTTrainingArgs, train_sft\n",
    "\n",
    "# The Datasets\n",
    "from mlx_lm_lora.trainer.datasets import CacheDataset, GRPODataset, TextDataset\n",
    "\n",
    "# The reward functions\n",
    "from mlx_lm_lora.trainer.grpo_reward_functions import (\n",
    "    r1_accuracy_reward_func,\n",
    "    r1_int_reward_func,\n",
    "    r1_strict_format_reward_func,\n",
    "    r1_soft_format_reward_func,\n",
    "    r1_count_xml,\n",
    ")\n",
    "\n",
    "# For loading/saving the model and calculating the steps\n",
    "from mlx_lm_lora.utils import from_pretrained, fuse_and_save_model, calculate_iters\n",
    "\n",
    "# For loading the dataset\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# Other needed stuff\n",
    "from mlx_lm.tuner.utils import print_trainable_parameters\n",
    "from mlx_lm.tuner.callbacks import TrainingCallback\n",
    "from mlx_lm.sample_utils import make_sampler\n",
    "from mlx_lm.generate import generate\n",
    "from mlx_lm.utils import save_config\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# The optimizer\n",
    "import mlx.optimizers as optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08959144",
   "metadata": {},
   "source": [
    "# Set the datasets, models, and loading params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ccaac3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name = \"Qwen/Qwen3-1.7B-Base\"\n",
    "zero_ref_model_name = \"Qwen/Qwen3-1.7B-Base\"\n",
    "zero_adapter_path = \"./Qwen3-1.7B-Zero\"\n",
    "zero_dataset_name = \"mlx-community/gsm8k\"\n",
    "r1_dataset_generator_model_name = \"Qwen/Qwen3-1.7B\"\n",
    "r1_model_name = \"Qwen/Qwen3-1.7B\"\n",
    "r1_adapter_path = \"./Qwen3-1.7B-R1\"\n",
    "num_r1_samples = 10 # How many reasoning samples we will generate the finetune the R1 model.\n",
    "\n",
    "max_seq_length = 1024\n",
    "lora_config = { # LoRA adapter configuration\n",
    "    \"rank\": 8,  # Low-rank bottleneck size (Larger rank = smarter, but slower). Suggested 8, 16, 32, 64, 128\n",
    "    \"dropout\": 0.0,\n",
    "    \"scale\": 10.0, # Multiplier for how hard the LoRA update hits the base weights\n",
    "    \"use_dora\": False,\n",
    "    \"num_layers\": -1 # Use -1 for all layers\n",
    "}\n",
    "quantized_config={\n",
    "    \"bits\": 4, # Use 4 bit quantization. Suggested 4, 6, 8\n",
    "    \"group_size\": 64\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2658e61c",
   "metadata": {},
   "source": [
    "# Let's first start with the zero model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e11f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model Qwen/Qwen3-1.7B-Base\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a442693323b74efabd3e0ec5f02260b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantizing model with 4 bits\n",
      "Loading model Qwen/Qwen3-1.7B-Base\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "203fd468c2a94d7eaf41fb45240c3474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LoRA adapters with config: {'rank': 8, 'dropout': 0.0, 'scale': 10.0, 'use_dora': False, 'num_layers': -1}\n",
      "Quantizing model with 4 bits\n",
      "Trainable parameters: 0.507% (8.716M/1720.575M)\n"
     ]
    }
   ],
   "source": [
    "zero_ref_model, zero_ref_tokenizer = from_pretrained(\n",
    "    model=zero_ref_model_name,\n",
    "    quantized_load=quantized_config,\n",
    ")\n",
    "\n",
    "zero_model, zero_tokenizer = from_pretrained(\n",
    "    model=base_model_name,\n",
    "    lora_config=lora_config,\n",
    "    quantized_load=quantized_config,\n",
    ")\n",
    "print_trainable_parameters(zero_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb1f3902",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_path = Path(zero_adapter_path)\n",
    "adapter_path.mkdir(parents=True, exist_ok=True)\n",
    "adapter_file = adapter_path / \"adapters.safetensors\"\n",
    "save_config(lora_config, adapter_path / \"adapter_config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fddb12",
   "metadata": {},
   "source": [
    "# Load and process the dataset\n",
    "\n",
    "We don't have to format the Dataset the GRPODataset class will do that itself.\n",
    "\n",
    "If you have to reformat before loading, keep in mind it should be a jsonl looking like:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"prompt\": \"...\",\n",
    "    \"answer\": \"...\"\n",
    "}\n",
    "```\n",
    "\n",
    "This model does not have the Prompt Format we want, so let's do that first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34fb10ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = \"\"\"\n",
    "{% if messages[0]['role'] == 'system' %}\n",
    "{{ messages[0]['content'] }}\n",
    "{% endif %}\n",
    "\n",
    "User: {{ messages[1]['content'] }}\n",
    "\n",
    "Assistant: \"\"\".strip()\n",
    "\n",
    "zero_tokenizer.chat_template = chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfcb9611",
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks quickly in the mind and then provides the user with the answer. The assistant places it's think process between <think> and </think> tags. Then, provides the raw solution between <answer> </answer> tags.\"\n",
    "\n",
    "train_set = GRPODataset(\n",
    "    load_dataset(zero_dataset_name)[\"train\"],\n",
    "    zero_tokenizer,\n",
    "    prompt_key=\"prompt\",\n",
    "    answer_key=\"answer\",\n",
    "    type_key=\"type\",\n",
    "    default_system_str=system\n",
    ")\n",
    "valid_set = GRPODataset(\n",
    "    load_dataset(zero_dataset_name)[\"valid\"],\n",
    "    zero_tokenizer,\n",
    "    prompt_key=\"prompt\",\n",
    "    answer_key=\"answer\",\n",
    "    type_key=\"type\",\n",
    "    default_system_str=system\n",
    ")\n",
    "test_set = GRPODataset(\n",
    "    load_dataset(zero_dataset_name)[\"test\"],\n",
    "    zero_tokenizer,\n",
    "    prompt_key=\"prompt\",\n",
    "    answer_key=\"answer\",\n",
    "    type_key=\"type\",\n",
    "    default_system_str=system\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbf62ac",
   "metadata": {},
   "source": [
    "# Let's see how the datasset looks like\n",
    "This is what will get inputed into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11ef39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input = zero_tokenizer.decode(test_set._data[0][0])\n",
    "print(sample_input)\n",
    "sample_input_answer = zero_tokenizer.decode(test_set._data[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27df97a4",
   "metadata": {},
   "source": [
    "Let's use this exact input the see what the untrained model generates. Since we know the actual answer to this question (18), we know how the model performs. Which is ok, the generated answer is correct!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840630ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_untrained_zero = generate(\n",
    "    model=zero_model,\n",
    "    tokenizer=zero_tokenizer,\n",
    "    prompt=sample_input,\n",
    "    max_tokens=max_seq_length//2,\n",
    ")\n",
    "\n",
    "print(test_untrained_zero)\n",
    "\n",
    "print(\"\\n\\n\" + \"-\"*100)\n",
    "print(f\"Actual answer: {sample_input_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d0bf58",
   "metadata": {},
   "source": [
    "# Now we're done with all the steps and can actually start the training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6792253d",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.AdamW(learning_rate=2e-4)  # Set the optimizer\n",
    "\n",
    "args = GRPOTrainingArgs(\n",
    "    batch_size=1,\n",
    "    iters=100, # calculate_iters(train_set=train_set, batch_size=1, epochs=1),\n",
    "    gradient_accumulation_steps=1,\n",
    "    val_batches=1,\n",
    "    steps_per_report=10,\n",
    "    steps_per_eval=100,\n",
    "    steps_per_save=200,\n",
    "    max_seq_length=max_seq_length,\n",
    "    adapter_file=adapter_file,\n",
    "    grad_checkpoint=True,\n",
    "    group_size=2,\n",
    "    beta=0.1,\n",
    "    epsilon=0.0001,\n",
    "    epsilon_high=0.1,\n",
    "    max_completion_length=max_seq_length//2,\n",
    "    reference_model_path=zero_ref_model_name,\n",
    "    temperature=0.6,\n",
    "    grpo_loss_type=\"grpo\", # Chosse one: \"grpo\", \"bnpo\", \"dr_grpo\"\n",
    "    reward_weights=None,\n",
    "    importance_sampling_level=\"sequence\", # Choose one: \"token\", \"sequence\", None\n",
    ")\n",
    "\n",
    "train_grpo(\n",
    "    model=zero_model,\n",
    "    tokenizer=zero_tokenizer,\n",
    "    ref_model=zero_ref_model.freeze(),\n",
    "    args=args,\n",
    "    optimizer=opt,\n",
    "    train_dataset=CacheDataset(train_set),\n",
    "    val_dataset=CacheDataset(valid_set),\n",
    "    training_callback=TrainingCallback(),\n",
    "    reward_funcs=[r1_accuracy_reward_func, r1_int_reward_func, r1_strict_format_reward_func, r1_soft_format_reward_func, r1_count_xml],\n",
    "    end_answer_token=\"</answer>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c94feb",
   "metadata": {},
   "source": [
    "# After training, let's evaluate and test the trained model out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392a0d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, _, rewards = evaluate_grpo(\n",
    "    model=zero_model,\n",
    "    tokenizer=zero_tokenizer,\n",
    "    ref_model=zero_ref_model.freeze(),\n",
    "    dataset=CacheDataset(test_set),\n",
    "    batch_size=1,\n",
    "    num_batches=1,\n",
    "    max_seq_length=max_seq_length,\n",
    "    beta=0.01,\n",
    "    epsilon=0.1,\n",
    "    epsilon_high=0.3,\n",
    "    group_size=1,\n",
    "    max_tokens=max_seq_length//2,\n",
    "    temperature=0.6,\n",
    "    reward_funcs=[\n",
    "        r1_accuracy_reward_func,\n",
    "        r1_int_reward_func,\n",
    "        r1_strict_format_reward_func,\n",
    "        r1_soft_format_reward_func,\n",
    "        r1_count_xml\n",
    "    ],\n",
    "    grpo_loss_type=\"grpo\",\n",
    "    importance_sampling_level=\"sequence\",\n",
    "    end_answer_token=\"</answer>\"\n",
    ")\n",
    "print(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1963ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_trained_zero = generate(\n",
    "    model=zero_model,\n",
    "    tokenizer=zero_tokenizer,\n",
    "    prompt=sample_input,\n",
    "    max_tokens=max_seq_length//2,\n",
    ")\n",
    "\n",
    "print(test_trained_zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ee0efb",
   "metadata": {},
   "source": [
    "# Finally let's merge and save the final zero model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ffe978",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuse_and_save_model(\n",
    "    model=zero_model,\n",
    "    tokenizer=zero_tokenizer,\n",
    "    save_path=adapter_path,\n",
    "    de_quantize=True # Since we quantized the model on load\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe746168",
   "metadata": {},
   "source": [
    "# Let's also remove the reference model from RAM, we don't need it anymore\n",
    "\n",
    "So that we free out some RAM before we continue..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "302c4fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "del zero_ref_model\n",
    "del zero_ref_tokenizer\n",
    "del valid_set\n",
    "del test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124075cb",
   "metadata": {},
   "source": [
    "# Dataset Curation Phase\n",
    "\n",
    "Now we can go into the dataset curation phase. Here we will first generate some reasoning traces using the zero model, after we've collected a sufficient number of traces, we need to distill them into a format suitable for SFT training.\n",
    "\n",
    "## Why Distillation?\n",
    "\n",
    "The zero model outputs structured responses with raw answers:\n",
    "```\n",
    "<think>\n",
    "reasoning steps\n",
    "</think>\n",
    "<answer> raw answer </answer>.\n",
    "```\n",
    "\n",
    "We want to transform this into natural language while preserving the reasoning:\n",
    "```\n",
    "<think> reasoning steps </think>\n",
    "fluent natural language answer\n",
    "```\n",
    "\n",
    "## Distillation Process\n",
    "\n",
    "We'll use a strong base model to rewrite the raw answers into natural language. This creates high-quality SFT data that teaches the model to:\n",
    "1. Maintain the reasoning process (thinking tags)\n",
    "2. Output polished, fluent answers\n",
    "3. Preserve correctness from the RL training\n",
    "\n",
    "### Step 1: Generate Zero Reasoning Traces\n",
    "We'll sample from our dataset, format prompts with the chat template, and generate some reasoning traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd55a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating trace 1/10...\n",
      "Generating trace 2/10...\n",
      "Generating trace 3/10...\n",
      "Generating trace 4/10...\n",
      "Generating trace 5/10...\n",
      "Generating trace 6/10...\n",
      "Generating trace 7/10...\n",
      "Generating trace 8/10...\n",
      "Generating trace 9/10...\n",
      "Generating trace 10/10...\n",
      "\n",
      "✓ Generated 10 zero reasoning traces\n"
     ]
    }
   ],
   "source": [
    "distil_dataset = load_dataset(zero_dataset_name)[\"train\"].select(range(num_r1_samples))\n",
    "zero_reasoning_traces = []\n",
    "prompts = []\n",
    "\n",
    "sampler = make_sampler(\n",
    "    temp=0.6,\n",
    "    top_p=0.95,\n",
    "    min_p=0.05,\n",
    "    top_k=20,\n",
    ")\n",
    "\n",
    "for idx in range(num_r1_samples):\n",
    "    example = distil_dataset[idx]\n",
    "    print(f\"Generating trace {idx+1}/{num_r1_samples}...\")\n",
    "\n",
    "    # Extract prompt\n",
    "    prompt_str = example[\"prompt\"]\n",
    "\n",
    "    # Format with chat template → returns input_ids\n",
    "    prompt_input = zero_tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\", \"content\": example[\"prompt\"]},\n",
    "        ],\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=False, # <- since we\"re using a qwen model which is a hybrid.\n",
    "    )\n",
    "\n",
    "    # Generate\n",
    "    response = generate(\n",
    "        model=zero_model,\n",
    "        tokenizer=zero_tokenizer,\n",
    "        prompt=prompt_input,\n",
    "        max_tokens=max_seq_length // 2,\n",
    "        sampler=sampler,\n",
    "    )\n",
    "\n",
    "    prompts.append(prompt_str)\n",
    "    zero_reasoning_traces.append(response)\n",
    "\n",
    "print(f\"\\n✓ Generated {len(zero_reasoning_traces)} zero reasoning traces\")\n",
    "\n",
    "with open(f\"{zero_adapter_path}/zero_reasoning_traces.json\", \"w\") as f:\n",
    "    json.dump(\n",
    "        {\n",
    "            \"prompts\": prompts,\n",
    "            \"traces\": zero_reasoning_traces\n",
    "        },\n",
    "        f,\n",
    "        indent=2\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989677f7",
   "metadata": {},
   "source": [
    "# Great lets take a lott at one of the generated traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6539698d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- \n",
      " Prompt: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? \n",
      " Generation: <think>\n",
      "1. First, Natalia sold clips to 48 of her friends in April, so she sold 48 clips in April.\n",
      "2. In May, Natalia sold half as many clips as in April, so she sold 48 / 2 = 24 clips in May.\n",
      "3. To find out how many clips Natalia sold altogether in April and May, we add the number of clips sold in April and May: 48 + 24 = 72.\n",
      "</think>\n",
      "\n",
      "<answer>72</answer>\n"
     ]
    }
   ],
   "source": [
    "print(\"-\"*500, \"\\n\", f\"Prompt: {prompts[0]}\", \"\\n\", f\"Generation: {zero_reasoning_traces[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ca4681e",
   "metadata": {},
   "outputs": [],
   "source": [
    "del zero_model\n",
    "del zero_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd882d2",
   "metadata": {},
   "source": [
    "### Step 2: Distill to Natural Language\n",
    "\n",
    "Now we'll use a strong model to rewrite the raw answers into fluent natural language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36950a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model Qwen/Qwen3-1.7B\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e795b1d0b615458e962b8d1d05eb464d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4fdf28a8793403f9c42fbfff20a3124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/726 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27900991a6dc40be8da664a31708f80d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "457554f0da2e4a3a9d5a459b90d5052f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "558fec02f2644bc29aa8991fbdfde6d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/622M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ddd0284a1324bd1ab896054709b2813",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "417222552e6342838e6c202ceb5160ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/3.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4025f70f182e4889826705e1b7a23c5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bf3adc3efa74190b69b77793cd10e68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a89298973e7c4094ab785898b9b6f3c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "distill_model, distill_tokenizer = from_pretrained(\n",
    "    model=r1_dataset_generator_model_name,\n",
    "    quantized_load=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12409836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_between(text, start_tag, end_tag):\n",
    "    \"\"\"Extract content between tags.\"\"\"\n",
    "    start_idx = text.find(start_tag)\n",
    "    end_idx = text.find(end_tag)\n",
    "    if start_idx == -1 or end_idx == -1:\n",
    "        return None\n",
    "    return text[start_idx + len(start_tag):end_idx].strip()\n",
    "\n",
    "def distill_trace(trace, model, tokenizer):\n",
    "    \"\"\"Convert one zero trace to SFT format with natural language answer.\"\"\"\n",
    "    \n",
    "    # Extract reasoning and raw answer\n",
    "    reasoning = extract_between(trace, \"<think>\", \"</think>\")\n",
    "    raw_answer = extract_between(trace, \"<answer>\", \"</answer>\")\n",
    "    \n",
    "    if not reasoning or not raw_answer:\n",
    "        return None\n",
    "    \n",
    "    # Rewrite raw answer to natural language\n",
    "    distill_prompt = f\"\"\"Given this reasoning and answer, rewrite the answer in clear, natural language. Only return the natural answer, no additional text:\n",
    "\n",
    "Reasoning: {reasoning}\n",
    "Raw answer: {raw_answer}\n",
    "\n",
    "Natural answer:\"\"\"\n",
    "    \n",
    "    sampler = make_sampler(\n",
    "        temp=0.8,\n",
    "        top_p=0.95,\n",
    "        min_p=0.0,\n",
    "        top_k=20,\n",
    "    )\n",
    "\n",
    "    distil_input = distill_tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": distill_prompt},\n",
    "        ],\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=False,\n",
    "        enable_thinking=False\n",
    "    )\n",
    "    \n",
    "    natural_answer = generate(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        prompt=distil_input,\n",
    "        max_tokens=max_seq_length,\n",
    "        sampler=sampler,\n",
    "    )\n",
    "    \n",
    "    sft_completion = f\"<think>\\n{reasoning}\\n</think>\\n{natural_answer.strip()}\"\n",
    "    \n",
    "    return sft_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875f6352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distilling 1/10...\n",
      "<think>\n",
      "1. First, Natalia sold clips to 48 of her friends in April, so she sold 48 clips in April.\n",
      "2. In May, Natalia sold half as many clips as in April, so she sold 48 / 2 = 24 clips in May.\n",
      "3. To find out how many clips Natalia sold altogether in April and May, we add the number of clips sold in April and May: 48 + 24 = 72.\n",
      "</think>\n",
      "Natalia sold 48 clips in April and 24 clips in May, totaling 72 clips.\n",
      "Distilling 2/10...\n",
      "Distilling 3/10...\n",
      "Distilling 4/10...\n",
      "<think>\n",
      "Yesterday, Julie read 12 pages, and today she read twice as many, which is 12 x 2 = 24 pages. In total, she has read 12 + 24 = 36 pages so far. The remaining pages are 120 - 36 = 84 pages. If she wants to read half of the remaining pages tomorrow, she needs to read 84 / 2 = 42 pages.\n",
      "</think>\n",
      "42\n",
      "Distilling 5/10...\n",
      "Distilling 6/10...\n",
      "<think>\n",
      "First, calculate the number of purple flowers by taking 10 flowers (yellow) and multiplying it by 1.80 (80% more). This gives 18 purple flowers.\n",
      "Next, calculate the number of green flowers by taking 10 flowers (yellow) and multiplying it by 0.25 (25% as many). This gives 2.5 green flowers.\n",
      "Now, add the number of yellow, purple, and green flowers together to get the total number of flowers in the garden.\n",
      "10 + 18 + 2.5 = 30.5\n",
      "The final answer is 30.5.\n",
      "</think>\n",
      "There are 30.5 flowers in the garden.\n",
      "Distilling 7/10...\n",
      "<think>\n",
      "First, we need to calculate the total number of slices from the large pizzas and the small pizzas. Then, we can find out the total number of slices Albert will eat if he eats every slice from both pizzas in one day.\n",
      "</think>\n",
      "Albert will eat a total of 192 pizza slices in a day.\n",
      "Distilling 8/10...\n",
      "Distilling 9/10...\n",
      "<think>\n",
      "First, I need to calculate the total amount of money Alexis had after leaving the store. She started with a budget of $200 and had $16 left after her purchases. So, she spent $200 - $16 = $184 on her clothes and shoes. Next, I need to add up the cost of the items she already purchased: $30 on a button-up shirt, $46 on suit pants, $38 on a suit coat, $11 on socks, and $18 on a belt. That's $30 + $46 + $38 + $11 + $18 = $143. To find out how much she paid for the shoes, I need to subtract the total cost of the other items from the amount she had after leaving the store: $184 - $143 = $41. So, Alexis paid $41 for the shoes.\n",
      "</think>\n",
      "Alexis paid $41 for the shoes.\n",
      "Distilling 10/10...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './Qwen3-1.7B-R1/sft_dataset.jsonl'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✓ Distilled \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m traces\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Save as JSONL (one JSON object per line)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mr1_adapter_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/sft_dataset.jsonl\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mw\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     20\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m sft_dataset:\n\u001b[32m     21\u001b[39m         f.write(json.dumps(item) + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mlx-lm-lora/.conda/lib/python3.11/site-packages/IPython/core/interactiveshell.py:326\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    321\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    322\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    324\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: './Qwen3-1.7B-R1/sft_dataset.jsonl'"
     ]
    }
   ],
   "source": [
    "sft_dataset = []\n",
    "for idx, (prompt, trace) in enumerate(zip(prompts, zero_reasoning_traces)):\n",
    "    print(f\"Distilling {idx+1}/{len(zero_reasoning_traces)}...\")\n",
    "    sft_completion = distill_trace(trace, distill_model, distill_tokenizer)\n",
    "    if sft_completion:\n",
    "        # Format as messages structure\n",
    "        sft_dataset.append({\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "                {\"role\": \"assistant\", \"content\": sft_completion}\n",
    "            ]\n",
    "        })\n",
    "        if (idx + 1) % 10 == 0:\n",
    "            print(f\"✓ Distilled {idx+1} traces\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "18c5a9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as JSONL (one JSON object per line)\n",
    "with open(\"./sft_dataset.jsonl\", \"w\") as f:\n",
    "    for item in sft_dataset:\n",
    "        f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0275056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
      "<think>\n",
      "1. First, Natalia sold clips to 48 of her friends in April, so she sold 48 clips in April.\n",
      "2. In May, Natalia sold half as many clips as in April, so she sold 48 / 2 = 24 clips in May.\n",
      "3. To find out how many clips Natalia sold altogether in April and May, we add the number of clips sold in April and May: 48 + 24 = 72.\n",
      "</think>\n",
      "Natalia sold 48 clips in April and 24 clips in May, for a total of 72 clips.\n"
     ]
    }
   ],
   "source": [
    "print(sft_dataset[0][\"prompt\"])\n",
    "print(sft_dataset[0][\"completion\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b4853c",
   "metadata": {},
   "source": [
    "### Step 3: Save Final SFT Dataset\n",
    "\n",
    "Yay! the dataset has been generated let's look at how it turned out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc23831c",
   "metadata": {},
   "outputs": [],
   "source": [
    "del distill_model\n",
    "del distill_tokenizer\n",
    "del distil_dataset\n",
    "del distill_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c022a519",
   "metadata": {},
   "source": [
    "# OK so now that we have our R1 dataset we can now SFT finetune the Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d4a58168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model Qwen/Qwen3-1.7B-Base\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b67bb83b58f43d0998e95a2d5d3ac2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LoRA adapters with config: {'rank': 8, 'dropout': 0.0, 'scale': 10.0, 'use_dora': False, 'num_layers': -1}\n",
      "Quantizing model with 4 bits\n"
     ]
    }
   ],
   "source": [
    "r1_model, r1_tokenizer = from_pretrained(\n",
    "    model=base_model_name,\n",
    "    lora_config=lora_config,\n",
    "    quantized_load=quantized_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4d0bd63a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3303684493e743c8b43dd3a0d3ceeffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "158238a6840449648a9604608f1bd54b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def format_prompts_func(sample):\n",
    "    sample[\"text\"] = r1_tokenizer.apply_chat_template(\n",
    "        conversation=sample[\"messages\"],\n",
    "        add_generation_prompt=False,\n",
    "        tokenize=False\n",
    "    )\n",
    "    return sample\n",
    "\n",
    "dataset = Dataset.from_list(sft_dataset) # Turn it into a pyarrow.Table to make Dataset class happy\n",
    "\n",
    "train_set = TextDataset(\n",
    "    dataset.map(format_prompts_func, ).remove_columns([\"messages\"]),\n",
    "    r1_tokenizer,\n",
    "    text_key=\"text\",\n",
    ")\n",
    "\n",
    "valid_set = TextDataset(\n",
    "    dataset.map(format_prompts_func, ).remove_columns([\"messages\"]),\n",
    "    r1_tokenizer,\n",
    "    text_key=\"text\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8ac35da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "1. First, Natalia sold clips to 48 of her friends in April, so she sold 48 clips in April.\n",
      "2. In May, Natalia sold half as many clips as in April, so she sold 48 / 2 = 24 clips in May.\n",
      "3. To find out how many clips Natalia sold altogether in April and May, we add the number of clips sold in April and May: 48 + 24 = 72.\n",
      "</think>\n",
      "\n",
      "Natalia sold 48 clips in April and 24 clips in May, totaling 72 clips.<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(valid_set[0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3482ad88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Calculated 5 iterations from 1 epochs (dataset size: 5, batch size: 1)\n",
      "Starting training..., iters: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1: Val loss 0.680, Val took 0.403s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|████████  | 4/5 [00:05<00:01,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 5: Val loss 0.573, Val took 0.261s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:06<00:00,  1.33s/it, loss=0.742, it/s=8.346]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iter 5: loss 0.742, lr 2.000e-04, it/s 8.346, tok/s 174.091, trained_tok 1043, peak_mem 6.882GB\n",
      "Saved final weights to Qwen3-1.7B-R1/adapters.safetensors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "adapter_path = Path(r1_adapter_path)\n",
    "adapter_path.mkdir(parents=True, exist_ok=True)\n",
    "adapter_file = adapter_path / \"adapters.safetensors\"\n",
    "save_config(lora_config, adapter_path / \"adapter_config.json\")\n",
    "\n",
    "opt = optim.AdamW(learning_rate=2e-4)\n",
    "\n",
    "# Training settings\n",
    "args = SFTTrainingArgs(\n",
    "    batch_size=1,\n",
    "    iters=calculate_iters(train_set, batch_size=1, epochs=1),\n",
    "    gradient_accumulation_steps=1,\n",
    "    val_batches=1,\n",
    "    steps_per_report=50,\n",
    "    steps_per_eval=500,\n",
    "    steps_per_save=200,\n",
    "    max_seq_length=max_seq_length,\n",
    "    adapter_file=adapter_file,\n",
    "    grad_checkpoint=True,\n",
    ")\n",
    "\n",
    "# Start Training\n",
    "train_sft(\n",
    "    model=r1_model,\n",
    "    args=args,\n",
    "    optimizer=opt,\n",
    "    train_dataset=CacheDataset(train_set),\n",
    "    val_dataset=CacheDataset(valid_set),\n",
    "    training_callback=TrainingCallback(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fb7f0e",
   "metadata": {},
   "source": [
    "# Sooooo, finaly! we\"re finished\n",
    "\n",
    "We just creaed and trained our own Reasoning model completely from scratch.\n",
    "\n",
    "The only thing we now have to do is to save te R1 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ef55ce26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "De-quantizing model\n",
      "Created README.md in Qwen3-1.7B-R1\n"
     ]
    }
   ],
   "source": [
    "fuse_and_save_model(\n",
    "    model=r1_model,\n",
    "    tokenizer=r1_tokenizer,\n",
    "    save_path=adapter_path,\n",
    "    de_quantize=True # Since we quantized the model on load\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe5c262",
   "metadata": {},
   "source": [
    "## That's it!\n",
    "\n",
    "And we're done! You successfully trained your own custom model. You can updload it using the api package by HF. If you have any questions on MLX-LM-LoRA, or find any bugs, or need help, feel free to go to my [GitHub](https://github.com/Goekdeniz-Guelmez/mlx-lm-lora)!\n",
    "\n",
    "Cheers,\n",
    "Gökdeniz"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
