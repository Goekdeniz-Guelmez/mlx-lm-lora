{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7ca9b44",
   "metadata": {},
   "source": [
    "# Train a custom R1 model from scratch using MLX-LM-LoRA\n",
    "\n",
    "In this one we will train a Zero model with the GRPO trainer to then create a reasoning dataset to then finaly train a custom R1 model. Grab some popcorn and enjoy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee5f7bf",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install -U mlx-lm-lora mlx-lm ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac842fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The trainers and evaluations\n",
    "from mlx_lm_lora.trainer.grpo_trainer import GRPOTrainingArgs, train_grpo, evaluate_grpo\n",
    "from mlx_lm_lora.trainer.sft_trainer import SFTTrainingArgs, train_sft\n",
    "\n",
    "# The Datasets\n",
    "from mlx_lm_lora.trainer.datasets import CacheDataset, GRPODataset, TextDataset\n",
    "\n",
    "# The reward functions\n",
    "from mlx_lm_lora.trainer.grpo_reward_functions import (\n",
    "    r1_accuracy_reward_func,\n",
    "    r1_int_reward_func,\n",
    "    r1_strict_format_reward_func,\n",
    "    r1_soft_format_reward_func,\n",
    "    r1_count_xml,\n",
    ")\n",
    "\n",
    "# For loading/saving the model and calculating the steps\n",
    "from mlx_lm_lora.utils import from_pretrained, fuse_and_save_model, calculate_iters\n",
    "\n",
    "# For loading the dataset\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# Other needed stuff\n",
    "from mlx_lm.tuner.utils import print_trainable_parameters\n",
    "from mlx_lm.tuner.callbacks import TrainingCallback\n",
    "from mlx_lm.sample_utils import make_sampler\n",
    "from mlx_lm.generate import generate\n",
    "from mlx_lm.utils import save_config\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# The optimizer\n",
    "import mlx.optimizers as optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08959144",
   "metadata": {},
   "source": [
    "# Set the datasets, models, and loading params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccaac3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name = \"Qwen/Qwen3-1.7B-Base\"\n",
    "zero_ref_model_name = \"Qwen/Qwen3-1.7B-Base\"\n",
    "zero_adapter_path = \"./Qwen3-1.7B-Zero\"\n",
    "zero_dataset_name = \"mlx-community/gsm8k\"\n",
    "r1_dataset_generator_model_name = \"Qwen/Qwen3-1.7B\"\n",
    "r1_model_name = \"Qwen/Qwen3-1.7B\"\n",
    "r1_adapter_path = \"./Qwen3-1.7B-R1\"\n",
    "num_r1_samples = 10 # How many reasoning samples we will generate the finetune the R1 model.\n",
    "\n",
    "max_seq_length = 512\n",
    "lora_config = { # LoRA adapter configuration\n",
    "    \"rank\": 8,  # Low-rank bottleneck size (Larger rank = smarter, but slower). Suggested 8, 16, 32, 64, 128\n",
    "    \"dropout\": 0.0,\n",
    "    \"scale\": 10.0, # Multiplier for how hard the LoRA update hits the base weights\n",
    "    \"use_dora\": False,\n",
    "    \"num_layers\": -1 # Use -1 for all layers\n",
    "}\n",
    "quantized_config={\n",
    "    \"bits\": 4, # Use 4 bit quantization. Suggested 4, 6, 8\n",
    "    \"group_size\": 64\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2658e61c",
   "metadata": {},
   "source": [
    "# Let's first start with the zero model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e11f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_ref_model, zero_ref_tokenizer = from_pretrained(\n",
    "    model=zero_ref_model_name,\n",
    "    quantized_load=quantized_config,\n",
    ")\n",
    "\n",
    "zero_model, zero_tokenizer = from_pretrained(\n",
    "    model=base_model_name,\n",
    "    lora_config=lora_config,\n",
    "    quantized_load=quantized_config,\n",
    ")\n",
    "print_trainable_parameters(zero_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1f3902",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_path = Path(zero_adapter_path)\n",
    "adapter_path.mkdir(parents=True, exist_ok=True)\n",
    "adapter_file = adapter_path / \"adapters.safetensors\"\n",
    "save_config(lora_config, adapter_path / \"adapter_config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fddb12",
   "metadata": {},
   "source": [
    "# Load and process the dataset\n",
    "\n",
    "We don't have to format the Dataset the GRPODataset class will do that itself.\n",
    "\n",
    "If you have to reformat before loading, keep in mind it should be a jsonl looking like:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"prompt\": \"...\",\n",
    "    \"answer\": \"...\"\n",
    "}\n",
    "```\n",
    "\n",
    "This model does not have the Prompt Format we want, so let's do that first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fb10ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = \"\"\"\n",
    "{% if messages[0]['role'] == 'system' %}\n",
    "{{ messages[0]['content'] }}\n",
    "{% endif %}\n",
    "\n",
    "User: {{ messages[1]['content'] }}\n",
    "\n",
    "Assistant: \"\"\".strip()\n",
    "\n",
    "zero_tokenizer.chat_template = chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcb9611",
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks quickly in the mind and then provides the user with the answer. The assistant places it's think process between <think> and </think> tags. Then, provides the raw solution between <answer> </answer> tags.\"\n",
    "\n",
    "train_set = GRPODataset(\n",
    "    load_dataset(zero_dataset_name)[\"train\"],\n",
    "    zero_tokenizer,\n",
    "    prompt_key=\"prompt\",\n",
    "    answer_key=\"answer\",\n",
    "    type_key=\"type\",\n",
    "    default_system_str=system\n",
    ")\n",
    "valid_set = GRPODataset(\n",
    "    load_dataset(zero_dataset_name)[\"valid\"],\n",
    "    zero_tokenizer,\n",
    "    prompt_key=\"prompt\",\n",
    "    answer_key=\"answer\",\n",
    "    type_key=\"type\",\n",
    "    default_system_str=system\n",
    ")\n",
    "test_set = GRPODataset(\n",
    "    load_dataset(zero_dataset_name)[\"test\"],\n",
    "    zero_tokenizer,\n",
    "    prompt_key=\"prompt\",\n",
    "    answer_key=\"answer\",\n",
    "    type_key=\"type\",\n",
    "    default_system_str=system\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbf62ac",
   "metadata": {},
   "source": [
    "# Let's see how the datasset looks like\n",
    "This is what will get inputed into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11ef39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input = zero_tokenizer.decode(test_set._data[0][0])\n",
    "print(sample_input)\n",
    "sample_input_answer = zero_tokenizer.decode(test_set._data[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27df97a4",
   "metadata": {},
   "source": [
    "Let's use this exact input the see what the untrained model generates. Since we know the actual answer to this question (18), we know how the model performs. Which is ok, the generated answer is correct!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840630ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_untrained_zero = generate(\n",
    "    model=zero_model,\n",
    "    tokenizer=zero_tokenizer,\n",
    "    prompt=sample_input,\n",
    "    max_tokens=max_seq_length//2,\n",
    ")\n",
    "\n",
    "print(test_untrained_zero)\n",
    "\n",
    "print(\"\\n\\n\" + \"-\"*100)\n",
    "print(f\"Actual answer: {sample_input_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d0bf58",
   "metadata": {},
   "source": [
    "# Now we're done with all the steps and can actually start the training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6792253d",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.AdamW(learning_rate=2e-4)  # Set the optimizer\n",
    "\n",
    "args = GRPOTrainingArgs(\n",
    "    batch_size=1,\n",
    "    iters=100, # calculate_iters(train_set=train_set, batch_size=1, epochs=1),\n",
    "    gradient_accumulation_steps=1,\n",
    "    val_batches=1,\n",
    "    steps_per_report=10,\n",
    "    steps_per_eval=100,\n",
    "    steps_per_save=200,\n",
    "    max_seq_length=max_seq_length,\n",
    "    adapter_file=adapter_file,\n",
    "    grad_checkpoint=True,\n",
    "    group_size=2,\n",
    "    beta=0.1,\n",
    "    epsilon=0.0001,\n",
    "    epsilon_high=0.1,\n",
    "    max_completion_length=max_seq_length//2,\n",
    "    reference_model_path=zero_ref_model_name,\n",
    "    temperature=0.6,\n",
    "    grpo_loss_type=\"grpo\", # Chosse one: \"grpo\", \"bnpo\", \"dr_grpo\"\n",
    "    reward_weights=None,\n",
    "    importance_sampling_level=\"sequence\", # Choose one: \"token\", \"sequence\", None\n",
    ")\n",
    "\n",
    "train_grpo(\n",
    "    model=zero_model,\n",
    "    tokenizer=zero_tokenizer,\n",
    "    ref_model=zero_ref_model.freeze(),\n",
    "    args=args,\n",
    "    optimizer=opt,\n",
    "    train_dataset=CacheDataset(train_set),\n",
    "    val_dataset=CacheDataset(valid_set),\n",
    "    training_callback=TrainingCallback(),\n",
    "    reward_funcs=[r1_accuracy_reward_func, r1_int_reward_func, r1_strict_format_reward_func, r1_soft_format_reward_func, r1_count_xml],\n",
    "    end_answer_token=\"</answer>\"\n",
    ")\n",
    "\n",
    "# peak_mem 11.743GB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c94feb",
   "metadata": {},
   "source": [
    "# After training, let's evaluate and test the trained model out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392a0d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, _, rewards = evaluate_grpo(\n",
    "    model=zero_model,\n",
    "    tokenizer=zero_tokenizer,\n",
    "    ref_model=zero_ref_model.freeze(),\n",
    "    dataset=CacheDataset(test_set),\n",
    "    batch_size=1,\n",
    "    num_batches=1,\n",
    "    max_seq_length=max_seq_length,\n",
    "    beta=0.01,\n",
    "    epsilon=0.1,\n",
    "    epsilon_high=0.3,\n",
    "    group_size=1,\n",
    "    max_tokens=max_seq_length//2,\n",
    "    temperature=0.6,\n",
    "    reward_funcs=[\n",
    "        r1_accuracy_reward_func,\n",
    "        r1_int_reward_func,\n",
    "        r1_strict_format_reward_func,\n",
    "        r1_soft_format_reward_func,\n",
    "        r1_count_xml\n",
    "    ],\n",
    "    grpo_loss_type=\"grpo\",\n",
    "    importance_sampling_level=\"sequence\",\n",
    "    end_answer_token=\"</answer>\"\n",
    ")\n",
    "print(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1963ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_trained_zero = generate(\n",
    "    model=zero_model,\n",
    "    tokenizer=zero_tokenizer,\n",
    "    prompt=sample_input,\n",
    "    max_tokens=max_seq_length//2,\n",
    ")\n",
    "\n",
    "print(test_trained_zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ee0efb",
   "metadata": {},
   "source": [
    "# Finally let's merge and save the final zero model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ffe978",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuse_and_save_model(\n",
    "    model=zero_model,\n",
    "    tokenizer=zero_tokenizer,\n",
    "    save_path=adapter_path,\n",
    "    de_quantize=True # Since we quantized the model on load\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe746168",
   "metadata": {},
   "source": [
    "# Let's also remove the reference model from RAM, we don't need it anymore\n",
    "\n",
    "So that we free out some RAM before we continue..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302c4fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "del zero_ref_model\n",
    "del zero_ref_tokenizer\n",
    "del valid_set\n",
    "del test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124075cb",
   "metadata": {},
   "source": [
    "# Dataset Curation Phase\n",
    "\n",
    "Now we can go into the dataset curation phase. Here we will first generate some reasoning traces using the zero model, after we've collected a sufficient number of traces, we need to distill them into a format suitable for SFT training.\n",
    "\n",
    "## Why Distillation?\n",
    "\n",
    "The zero model outputs structured responses with raw answers:\n",
    "```\n",
    "<think>\n",
    "reasoning steps\n",
    "</think>\n",
    "<answer> raw answer </answer>.\n",
    "```\n",
    "\n",
    "We want to transform this into natural language while preserving the reasoning:\n",
    "```\n",
    "<think> reasoning steps </think>\n",
    "fluent natural language answer\n",
    "```\n",
    "\n",
    "## Distillation Process\n",
    "\n",
    "We'll use a strong base model to rewrite the raw answers into natural language. This creates high-quality SFT data that teaches the model to:\n",
    "1. Maintain the reasoning process (thinking tags)\n",
    "2. Output polished, fluent answers\n",
    "3. Preserve correctness from the RL training\n",
    "\n",
    "### Step 1: Generate Zero Reasoning Traces\n",
    "We'll sample from our dataset, format prompts with the chat template, and generate some reasoning traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd55a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "distil_dataset = load_dataset(zero_dataset_name)[\"train\"].select(range(num_r1_samples))\n",
    "zero_reasoning_traces = []\n",
    "prompts = []\n",
    "\n",
    "sampler = make_sampler(\n",
    "    temp=0.6,\n",
    "    top_p=0.95,\n",
    "    min_p=0.05,\n",
    "    top_k=20,\n",
    ")\n",
    "\n",
    "for idx in range(num_r1_samples):\n",
    "    example = distil_dataset[idx]\n",
    "    print(f\"Generating trace {idx+1}/{num_r1_samples}...\")\n",
    "\n",
    "    # Extract prompt\n",
    "    prompt_str = example[\"prompt\"]\n",
    "\n",
    "    # Format with chat template → returns input_ids\n",
    "    prompt_input = zero_tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\", \"content\": example[\"prompt\"]},\n",
    "        ],\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=False, # <- since we\"re using a qwen model which is a hybrid.\n",
    "    )\n",
    "\n",
    "    # Generate\n",
    "    response = generate(\n",
    "        model=zero_model,\n",
    "        tokenizer=zero_tokenizer,\n",
    "        prompt=prompt_input,\n",
    "        max_tokens=max_seq_length // 2,\n",
    "        sampler=sampler,\n",
    "    )\n",
    "\n",
    "    prompts.append(prompt_str)\n",
    "    zero_reasoning_traces.append(response)\n",
    "\n",
    "print(f\"\\n✓ Generated {len(zero_reasoning_traces)} zero reasoning traces\")\n",
    "\n",
    "with open(f\"{zero_adapter_path}/zero_reasoning_traces.json\", \"w\") as f:\n",
    "    json.dump(\n",
    "        {\n",
    "            \"prompts\": prompts,\n",
    "            \"traces\": zero_reasoning_traces\n",
    "        },\n",
    "        f,\n",
    "        indent=2\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989677f7",
   "metadata": {},
   "source": [
    "# Great lets take a lott at one of the generated traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6539698d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-\"*500, \"\\n\", f\"Prompt: {prompts[0]}\", \"\\n\", f\"Generation: {zero_reasoning_traces[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca4681e",
   "metadata": {},
   "outputs": [],
   "source": [
    "del zero_model\n",
    "del zero_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd882d2",
   "metadata": {},
   "source": [
    "### Step 2: Distill to Natural Language\n",
    "\n",
    "Now we'll use a strong model to rewrite the raw answers into fluent natural language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36950a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "distill_model, distill_tokenizer = from_pretrained(\n",
    "    model=r1_dataset_generator_model_name,\n",
    "    quantized_load=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12409836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_between(text, start_tag, end_tag):\n",
    "    \"\"\"Extract content between tags.\"\"\"\n",
    "    start_idx = text.find(start_tag)\n",
    "    end_idx = text.find(end_tag)\n",
    "    if start_idx == -1 or end_idx == -1:\n",
    "        return None\n",
    "    return text[start_idx + len(start_tag):end_idx].strip()\n",
    "\n",
    "def distill_trace(trace, model, tokenizer):\n",
    "    \"\"\"Convert one zero trace to SFT format with natural language answer.\"\"\"\n",
    "    \n",
    "    # Extract reasoning and raw answer\n",
    "    reasoning = extract_between(trace, \"<think>\", \"</think>\")\n",
    "    raw_answer = extract_between(trace, \"<answer>\", \"</answer>\")\n",
    "    \n",
    "    if not reasoning or not raw_answer:\n",
    "        return None\n",
    "    \n",
    "    # Rewrite raw answer to natural language\n",
    "    distill_prompt = f\"\"\"Given this reasoning and answer, rewrite the answer in clear, natural language. Only return the natural answer, no additional text:\n",
    "\n",
    "Reasoning: {reasoning}\n",
    "Raw answer: {raw_answer}\n",
    "\n",
    "Natural answer:\"\"\"\n",
    "    \n",
    "    sampler = make_sampler(\n",
    "        temp=0.8,\n",
    "        top_p=0.95,\n",
    "        min_p=0.0,\n",
    "        top_k=20,\n",
    "    )\n",
    "\n",
    "    distil_input = distill_tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": distill_prompt},\n",
    "        ],\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=False,\n",
    "        enable_thinking=False\n",
    "    )\n",
    "    \n",
    "    natural_answer = generate(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        prompt=distil_input,\n",
    "        max_tokens=max_seq_length,\n",
    "        sampler=sampler,\n",
    "    )\n",
    "    \n",
    "    sft_completion = f\"<think>\\n{reasoning}\\n</think>\\n{natural_answer.strip()}\"\n",
    "    \n",
    "    return sft_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875f6352",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_dataset = []\n",
    "for idx, (prompt, trace) in enumerate(zip(prompts, zero_reasoning_traces)):\n",
    "    print(f\"Distilling {idx+1}/{len(zero_reasoning_traces)}...\")\n",
    "    sft_completion = distill_trace(trace, distill_model, distill_tokenizer)\n",
    "    if sft_completion:\n",
    "        # Format as messages structure\n",
    "        sft_dataset.append({\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "                {\"role\": \"assistant\", \"content\": sft_completion}\n",
    "            ]\n",
    "        })\n",
    "        if (idx + 1) % 10 == 0:\n",
    "            print(f\"✓ Distilled {idx+1} traces\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c5a9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as JSONL (one JSON object per line)\n",
    "with open(\"./sft_dataset.jsonl\", \"w\") as f:\n",
    "    for item in sft_dataset:\n",
    "        f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0275056",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sft_dataset[0][\"prompt\"])\n",
    "print(sft_dataset[0][\"completion\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b4853c",
   "metadata": {},
   "source": [
    "### Step 3: Save Final SFT Dataset\n",
    "\n",
    "Yay! the dataset has been generated let's look at how it turned out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc23831c",
   "metadata": {},
   "outputs": [],
   "source": [
    "del distill_model\n",
    "del distill_tokenizer\n",
    "del distil_dataset\n",
    "del distill_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c022a519",
   "metadata": {},
   "source": [
    "# OK so now that we have our R1 dataset we can now SFT finetune the Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a58168",
   "metadata": {},
   "outputs": [],
   "source": [
    "r1_model, r1_tokenizer = from_pretrained(\n",
    "    model=base_model_name,\n",
    "    lora_config=lora_config,\n",
    "    quantized_load=quantized_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0bd63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompts_func(sample):\n",
    "    sample[\"text\"] = r1_tokenizer.apply_chat_template(\n",
    "        conversation=sample[\"messages\"],\n",
    "        add_generation_prompt=False,\n",
    "        tokenize=False\n",
    "    )\n",
    "    return sample\n",
    "\n",
    "dataset = Dataset.from_list(sft_dataset) # Turn it into a pyarrow.Table to make Dataset class happy\n",
    "\n",
    "train_set = TextDataset(\n",
    "    dataset.map(format_prompts_func, ).remove_columns([\"messages\"]),\n",
    "    r1_tokenizer,\n",
    "    text_key=\"text\",\n",
    ")\n",
    "\n",
    "valid_set = TextDataset(\n",
    "    dataset.map(format_prompts_func, ).remove_columns([\"messages\"]),\n",
    "    r1_tokenizer,\n",
    "    text_key=\"text\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac35da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(valid_set[0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3482ad88",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_path = Path(r1_adapter_path)\n",
    "adapter_path.mkdir(parents=True, exist_ok=True)\n",
    "adapter_file = adapter_path / \"adapters.safetensors\"\n",
    "save_config(lora_config, adapter_path / \"adapter_config.json\")\n",
    "\n",
    "opt = optim.AdamW(learning_rate=2e-4)\n",
    "\n",
    "# Training settings\n",
    "args = SFTTrainingArgs(\n",
    "    batch_size=1,\n",
    "    iters=calculate_iters(train_set, batch_size=1, epochs=1),\n",
    "    gradient_accumulation_steps=1,\n",
    "    val_batches=1,\n",
    "    steps_per_report=50,\n",
    "    steps_per_eval=500,\n",
    "    steps_per_save=200,\n",
    "    max_seq_length=max_seq_length,\n",
    "    adapter_file=adapter_file,\n",
    "    grad_checkpoint=True,\n",
    ")\n",
    "\n",
    "# Start Training\n",
    "train_sft(\n",
    "    model=r1_model,\n",
    "    args=args,\n",
    "    optimizer=opt,\n",
    "    train_dataset=CacheDataset(train_set),\n",
    "    val_dataset=CacheDataset(valid_set),\n",
    "    training_callback=TrainingCallback(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fb7f0e",
   "metadata": {},
   "source": [
    "# Sooooo, finaly! we\"re finished\n",
    "\n",
    "We just creaed and trained our own Reasoning model completely from scratch.\n",
    "\n",
    "The only thing we now have to do is to save te R1 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef55ce26",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuse_and_save_model(\n",
    "    model=r1_model,\n",
    "    tokenizer=r1_tokenizer,\n",
    "    save_path=adapter_path,\n",
    "    de_quantize=True # Since we quantized the model on load\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe5c262",
   "metadata": {},
   "source": [
    "## That's it!\n",
    "\n",
    "And we're done! You successfully trained your own custom model. You can updload it using the api package by HF. If you have any questions on MLX-LM-LoRA, or find any bugs, or need help, feel free to go to my [GitHub](https://github.com/Goekdeniz-Guelmez/mlx-lm-lora)!\n",
    "\n",
    "Cheers,\n",
    "Gökdeniz"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx-lm-lora-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
