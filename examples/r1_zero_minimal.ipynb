{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7ca9b44",
   "metadata": {},
   "source": [
    "# Train a custom reasoning model using MLX-LM-LoRA's GRPO trainer\n",
    "\n",
    "I'm about to demonstrate the power of MLX-LM-LoRA through a RL example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee5f7bf",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install -U mlx-lm-lora mlx-lm ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac842fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The trainer and evaluations\n",
    "from mlx_lm_lora.trainer.grpo_trainer import GRPOTrainingArgs, train_grpo, evaluate_grpo\n",
    "\n",
    "# The Datasets\n",
    "from mlx_lm_lora.trainer.datasets import CacheDataset, GRPODataset\n",
    "\n",
    "# The reward functions\n",
    "from mlx_lm_lora.trainer.grpo_reward_functions import (\n",
    "    r1_accuracy_reward_func,\n",
    "    r1_int_reward_func,\n",
    "    r1_strict_format_reward_func,\n",
    "    r1_soft_format_reward_func,\n",
    "    r1_count_xml\n",
    ")\n",
    "\n",
    "# For loading/saving the model and calculating the steps\n",
    "from mlx_lm_lora.utils import from_pretrained, fuse_and_save_model, calculate_iters\n",
    "\n",
    "# For loading the dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Other needed stuff\n",
    "from mlx_lm.tuner.utils import print_trainable_parameters\n",
    "from mlx_lm.tuner.callbacks import TrainingCallback\n",
    "from mlx_lm.generate import generate\n",
    "from mlx_lm.utils import save_config\n",
    "from pathlib import Path\n",
    "\n",
    "# The optimizer\n",
    "import mlx.optimizers as optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08959144",
   "metadata": {},
   "source": [
    "# Set the datase, model, and loading params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccaac3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mistralai/Ministral-3-3B-Base-2512\"\n",
    "ref_model_name = \"mistralai/Ministral-3-3B-Base-2512\"\n",
    "adapter_path = \"./Ministral-3-3B-Zero\"\n",
    "dataset_name = \"mlx-community/Dolci-Think-RL-7B-2k\"\n",
    "\n",
    "max_seq_length = 4096\n",
    "lora_config = { # LoRA adapter configuration\n",
    "    \"rank\": 8,  # Low-rank bottleneck size (Larger rank = smarter, but slower). Suggested 8, 16, 32, 64, 128\n",
    "    \"dropout\": 0.0,\n",
    "    \"scale\": 10.0, # Multiplier for how hard the LoRA update hits the base weights\n",
    "    \"use_dora\": False,\n",
    "    \"num_layers\": 8 # Use -1 for all layers\n",
    "}\n",
    "quantized_config={\n",
    "    \"bits\": 4, # Use 4 bit quantization. Suggested 4, 6, 8\n",
    "    \"group_size\": 64\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e11f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_model, _ = from_pretrained(\n",
    "    model=ref_model_name,\n",
    "    quantized_load=quantized_config, # Ref model shoudl be \"smarter\" then studend model\n",
    ")\n",
    "\n",
    "model, tokenizer = from_pretrained(\n",
    "    model=model_name,\n",
    "    lora_config=lora_config,\n",
    "    quantized_load=quantized_config,\n",
    ")\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1f3902",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_path = Path(adapter_path)\n",
    "adapter_path.mkdir(parents=True, exist_ok=True)\n",
    "adapter_file = adapter_path / \"adapters.safetensors\"\n",
    "save_config(lora_config, adapter_path / \"adapter_config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fddb12",
   "metadata": {},
   "source": [
    "# Load and process the dataset\n",
    "\n",
    "We don't have to format the Dataset the GRPODataset class will do that itself.\n",
    "\n",
    "If you have to reformat before loading, keep in mind it should be a jsonl looking like:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"prompt\": \"...\",\n",
    "    \"answer\": \"...\"\n",
    "}\n",
    "```\n",
    "\n",
    "This model does not have the Prompt Format we want, so let's do that first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fb10ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = \"\"\"\n",
    "{% if messages[0]['role'] == 'system' %}\n",
    "{{ messages[0]['content'] }}\n",
    "{% endif %}\n",
    "\n",
    "User: {{ messages[1]['content'] }}\n",
    "\n",
    "Assistant: Let me solve this step by step.\n",
    "\"\"\".strip()\n",
    "\n",
    "tokenizer.chat_template = chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcb9611",
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The assistant places it's reasoning between <think> and </think>. Then, provides the solution between <answer> </answer>.\"\n",
    "\n",
    "train_set = GRPODataset(\n",
    "    load_dataset(dataset_name)[\"train\"],\n",
    "    tokenizer,\n",
    "    prompt_key=\"prompt\",\n",
    "    answer_key=\"answer\",\n",
    "    type_key=\"type\",\n",
    "    default_system_str=system\n",
    ")\n",
    "valid_set = GRPODataset(\n",
    "    load_dataset(dataset_name)[\"valid\"],\n",
    "    tokenizer,\n",
    "    prompt_key=\"prompt\",\n",
    "    answer_key=\"answer\",\n",
    "    type_key=\"type\",\n",
    "    default_system_str=system\n",
    ")\n",
    "test_set = GRPODataset(\n",
    "    load_dataset(dataset_name)[\"test\"],\n",
    "    tokenizer,\n",
    "    prompt_key=\"prompt\",\n",
    "    answer_key=\"answer\",\n",
    "    type_key=\"type\",\n",
    "    default_system_str=system\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbf62ac",
   "metadata": {},
   "source": [
    "# Let's test how the datasset looks like\n",
    "This is what will get inputed into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11ef39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input = tokenizer.decode(test_set._data[0][0])\n",
    "print(sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27df97a4",
   "metadata": {},
   "source": [
    "Let's use this exact input the see what the untrained model generates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840630ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_untrained = generate(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    prompt=sample_input,\n",
    "    max_tokens=max_seq_length//4,\n",
    ")\n",
    "\n",
    "print(test_untrained)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d0bf58",
   "metadata": {},
   "source": [
    "# Now we're done with all the steps and can actually start the training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6792253d",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.Muon(learning_rate=2e-4)  # Set the optimizer\n",
    "\n",
    "args = GRPOTrainingArgs(\n",
    "    batch_size=1,\n",
    "    iters=calculate_iters(train_set=train_set, batch_size=1, epochs=1),\n",
    "    gradient_accumulation_steps=1,\n",
    "    val_batches=1,\n",
    "    steps_per_report=25,\n",
    "    steps_per_eval=100,\n",
    "    steps_per_save=200,\n",
    "    max_seq_length=max_seq_length,\n",
    "    adapter_file=adapter_file,\n",
    "    grad_checkpoint=True,\n",
    "    group_size=1,\n",
    "    beta=0.01,\n",
    "    epsilon=0.1,\n",
    "    epsilon_high=0.3,\n",
    "    max_completion_length=max_seq_length//2,\n",
    "    reference_model_path=ref_model_name,\n",
    "    temperature=0.6,\n",
    "    grpo_loss_type=\"grpo\", # Chosse one: \"grpo\", \"bnpo\", \"dr_grpo\"\n",
    "    reward_weights=None,\n",
    "    importance_sampling_level=None # Choose one: \"token\", \"sequence\", None\n",
    ")\n",
    "\n",
    "train_grpo(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    ref_model=ref_model.freeze(),\n",
    "    args=args,\n",
    "    optimizer=opt,\n",
    "    train_dataset=CacheDataset(train_set),\n",
    "    val_dataset=CacheDataset(valid_set),\n",
    "    training_callback=TrainingCallback(),\n",
    "    reward_funcs=[r1_accuracy_reward_func, r1_int_reward_func, r1_strict_format_reward_func, r1_soft_format_reward_func, r1_count_xml]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c94feb",
   "metadata": {},
   "source": [
    "# After training, let's evaluate and test the trained model out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392a0d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, _, rewards = evaluate_grpo(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    ref_model=ref_model.freeze(),\n",
    "    dataset=CacheDataset(test_set),\n",
    "    batch_size=1,\n",
    "    num_batches=1,\n",
    "    max_seq_length=max_seq_length,\n",
    "    beta=0.01,\n",
    "    epsilon=0.1,\n",
    "    epsilon_high=0.3,\n",
    "    group_size=1,\n",
    "    max_tokens=max_seq_length//2,\n",
    "    temperature=0.7,\n",
    "    reward_funcs=[\n",
    "        r1_accuracy_reward_func,\n",
    "        r1_int_reward_func,\n",
    "        r1_strict_format_reward_func,\n",
    "        r1_soft_format_reward_func,\n",
    "        r1_count_xml\n",
    "    ],\n",
    "    grpo_loss_type=\"grpo\",\n",
    "    importance_sampling_level=None\n",
    ")\n",
    "print(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1963ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_trained = generate(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    prompt=sample_input,\n",
    "    max_tokens=max_seq_length//2,\n",
    ")\n",
    "\n",
    "print(test_trained)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ee0efb",
   "metadata": {},
   "source": [
    "# Finally let's merge and save the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ffe978",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuse_and_save_model(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    save_path=adapter_path,\n",
    "    de_quantize=True # Since we quantized the model on load\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe5c262",
   "metadata": {},
   "source": [
    "## That's it!\n",
    "\n",
    "And we're done! You successfully trained your own custom model. You can updload it using the api package by HF. If you have any questions on MLX-LM-LoRA, or find any bugs, or need help, feel free to go to my [GitHub](https://github.com/Goekdeniz-Guelmez/mlx-lm-lora)!\n",
    "\n",
    "Cheers,\n",
    "GÃ¶kdeniz"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx-lm-lora-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
