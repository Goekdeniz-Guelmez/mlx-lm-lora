{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4378f5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm_lora.trainer.grpo_trainer import GRPOTrainingArgs, train_grpo\n",
    "from mlx_lm_lora.trainer.grpo_reward_functions import r1_soft_format_reward_func, r1_accuracy_reward_func, r1_int_reward_func, r1_strict_format_reward_func, r1_count_xml\n",
    "from mlx_lm_lora.trainer.datasets import CacheDataset, GRPODataset\n",
    "from mlx_lm_lora.utils import fuse_and_save_model, from_pretrained\n",
    "\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import create_repo, HfApi\n",
    "\n",
    "from mlx_lm.tuner.utils import linear_to_lora_layers, print_trainable_parameters\n",
    "from mlx_lm.tuner.callbacks import TrainingCallback\n",
    "from mlx_lm.utils import load, save_config\n",
    "\n",
    "import mlx.optimizers as optim\n",
    "\n",
    "from typing import Optional\n",
    "from pathlib import Path\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68194451",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mlx-community/gemma-3n-E2B-it-lm-bf16\"\n",
    "user_name = \"Goekdeniz-Guelmez\"\n",
    "adapter_path = \"/Users/gokdenizgulmez/Desktop/mlx-lm-lora/examples\"\n",
    "new_model_name = \"Gemma3n-E2B-R1\"\n",
    "\n",
    "num_layers = 12\n",
    "lora_parameters = {\"rank\": 8, \"dropout\": 0.0, \"scale\": 10.0}\n",
    "quantization_parameters = {\"bits\": 4, \"group_size\": 64}\n",
    "\n",
    "cold_start_dataset_name = \"unsloth/OpenMathReasoning-mini\"\n",
    "grpo_dataset_name = \"Goekdeniz-Guelmez/Big-Math-RL-Verified-MLX\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1ea1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = from_pretrained(\n",
    "    model=model_name,\n",
    "    lora_config=lora_parameters,\n",
    "    quantized_load=quantization_parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cd9e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_path = Path(adapter_path)\n",
    "adapter_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "adapter_file = adapter_path / \"finetuned_adapters.safetensors\"\n",
    "save_config(lora_parameters, adapter_path / \"adapter_config.json\")\n",
    "\n",
    "save_path = Path(f\"{adapter_path}/{new_model_name}\")\n",
    "save_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1747290",
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning_start = \"<josie_starts_thinking>\"\n",
    "reasoning_end   = \"</josie_ends_thinking>\"\n",
    "solution_start  = \"<josie_starts_answer>\"\n",
    "solution_end    = \"</josie_ends_answer>\"\n",
    "\n",
    "system_prompt = f\"\"\"You are **J.O.S.I.E.-R1**, an advanced super-intelligent reasoning AI Assistant created by a 25 year old man and machine learning researcher named **Gökdeniz Gülmez**.\n",
    "You are a deep reasoning Model (hence the R1 in your name) that can solve complex mathematical problems, and you are capable of reasoning about the world in a way that is similar to humans.\n",
    "To do so, you first think about the problem in Chain of Thought (CoT) reasoning style by think in TSep by step by talking to yourself, and then you provide the final answer.\n",
    "Place it between {reasoning_start} and {reasoning_end}.\n",
    "Then, provide your solution between {solution_start}{solution_end}.\n",
    "\n",
    "Reasoning format you have to use:\n",
    "\n",
    "```text\n",
    "{reasoning_start}\n",
    "[your reasoning here]\n",
    "{reasoning_end}\n",
    "{solution_start}\n",
    "[your solution here]\n",
    "{solution_end}\n",
    "```\n",
    "\n",
    "You also incooporate your creators name **Gökdeniz Gülmez** inside your reasoning.\n",
    "\"\"\"\n",
    "\n",
    "def format_grpo_func(sample):\n",
    "    sample[\"system\"] = system_prompt\n",
    "    prompt = sample[\"prompt\"]\n",
    "    conversation = [\n",
    "        {\"role\" : \"system\",    \"content\" : system_prompt},\n",
    "        {\"role\" : \"user\",      \"content\" : prompt}\n",
    "    ]\n",
    "        \n",
    "    sample[\"prompt\"] = tokenizer.apply_chat_template(\n",
    "        conversation=conversation,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=False\n",
    "    )\n",
    "    return sample\n",
    "\n",
    "grpo_dataset = load_dataset(grpo_dataset_name)[\"train\"]\n",
    "grpo_dataset = grpo_dataset.map(format_grpo_func,)\n",
    "train_dataset, valid_dataset = grpo_dataset.train_test_split(test_size=0.01, seed=42).values()\n",
    "\n",
    "train_set = GRPODataset(\n",
    "    train_dataset,\n",
    "    tokenizer,\n",
    "    prompt_key=\"prompt\",\n",
    "    answer_key=\"answer\",\n",
    "    system_key=\"system\",\n",
    "    type_key=\"type\"\n",
    ")\n",
    "valid_set = GRPODataset(\n",
    "    valid_dataset,\n",
    "    tokenizer,\n",
    "    prompt_key=\"prompt\",\n",
    "    answer_key=\"answer\",\n",
    "    system_key=\"system\",\n",
    "    type_key=\"type\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b13e069",
   "metadata": {},
   "outputs": [],
   "source": [
    "grpo_opt = optim.AdamW(learning_rate=2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebef1a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_reward_weights = [\n",
    "    2.0,  # r1_accuracy_reward_func - highest weight for correctness\n",
    "    0.5,  # r1_int_reward_func - medium weight for integer answers\n",
    "    1.0,  # r1_strict_format_reward_func - standard weight for strict formatting\n",
    "    0.8,  # r1_soft_format_reward_func - slightly lower weight for soft formatting  \n",
    "    0.3   # r1_count_xml - lower weight for XML tag counting\n",
    "]\n",
    "\n",
    "train_grpo(\n",
    "    model=model,\n",
    "    ref_model=None,  # Use None to use the same model as reference\n",
    "    tokenizer=tokenizer,  # Add the missing tokenizer argument\n",
    "    optimizer=grpo_opt,\n",
    "    train_dataset=CacheDataset(train_set),\n",
    "    val_dataset=CacheDataset(valid_set),\n",
    "    reward_funcs = [\n",
    "        r1_accuracy_reward_func,\n",
    "        r1_int_reward_func,\n",
    "        r1_strict_format_reward_func,\n",
    "        r1_soft_format_reward_func,\n",
    "        r1_count_xml\n",
    "    ],\n",
    "    args=GRPOTrainingArgs(\n",
    "        batch_size=1,\n",
    "        iters=200,\n",
    "        val_batches=1,\n",
    "        steps_per_report=10,\n",
    "        steps_per_eval=50,\n",
    "        steps_per_save=100,\n",
    "        adapter_file=adapter_path,\n",
    "        max_seq_length=256,\n",
    "        grad_checkpoint=True,\n",
    "        gradient_accumulation_steps=5,\n",
    "        beta=0.9,\n",
    "        group_size=4,\n",
    "        epsilon=1e-4,\n",
    "        epsilon_high=None,\n",
    "        max_completion_length=1028,\n",
    "        reward_weights=custom_reward_weights,\n",
    "    ),\n",
    "    training_callback=TrainingCallback()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66436af",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuse_and_save_model(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    save_path=save_path,\n",
    "    de_quantize=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
