{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65c9a94f",
   "metadata": {},
   "source": [
    "# Train a custom Chat model using MLX-LM-LoRA's SFT trainer\n",
    "\n",
    "I'm about to demonstrate the power of MLX-LM-LoRA through a finetuning example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b975dd80",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install -U mlx-lm-lora ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c886228",
   "metadata": {},
   "source": [
    "# Import the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5181f41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The trainer and evaluations\n",
    "from mlx_lm_lora.trainer.sft_trainer import SFTTrainingArgs, train_sft, evaluate_sft\n",
    "\n",
    "# The Datasets\n",
    "from mlx_lm_lora.trainer.datasets import CacheDataset, TextDataset\n",
    "\n",
    "# For loading/saving the model and calculating the steps\n",
    "from mlx_lm_lora.utils import from_pretrained, fuse_and_save_model, calculate_iters\n",
    "\n",
    "# For loading the dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Other needed stuff\n",
    "from mlx_lm.tuner.utils import print_trainable_parameters\n",
    "from mlx_lm.tuner.callbacks import TrainingCallback\n",
    "from mlx_lm.utils import save_config\n",
    "from mlx_lm.generate import generate\n",
    "from pathlib import Path\n",
    "\n",
    "# The optimizer\n",
    "import mlx.optimizers as optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b21bffe",
   "metadata": {},
   "source": [
    "# Set the datase, model, and loading params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae1b799",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen3-1.7B-Base\"\n",
    "new_model_name = \"Custom-Qwen3-1.7B\"\n",
    "adapter_path = \"./tests\"\n",
    "dataset_name = \"mlx-community/Dolci-Instruct-SFT-No-Tools-100K\"\n",
    "\n",
    "max_seq_length = 8192\n",
    "lora_config = { # LoRA adapter configuration\n",
    "    \"rank\": 8,  # Low-rank bottleneck size (Larger rank = smarter, but slower). Suggested 8, 16, 32, 64, 128\n",
    "    \"dropout\": 0.0,\n",
    "    \"scale\": 10.0, # Multiplier for how hard the LoRA update hits the base weights\n",
    "    \"use_dora\": False,\n",
    "    \"num_layers\": 8 # Use -1 for all layers\n",
    "}\n",
    "quantized_config={\n",
    "    \"bits\": 4, # Use 4 bit quantization. Suggested 4, 6, 8\n",
    "    \"group_size\": 64\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7858d64f",
   "metadata": {},
   "source": [
    "# Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a2fa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = from_pretrained(\n",
    "    model=model_name,\n",
    "    lora_config=lora_config, # None for no LoRA\n",
    "    quantized_load=quantized_config, # None for full bf16\n",
    ")\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d113624e",
   "metadata": {},
   "source": [
    "# Set the adapter path and file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dc3a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_path = Path(adapter_path)\n",
    "adapter_path.mkdir(parents=True, exist_ok=True)\n",
    "adapter_file = adapter_path / \"adapters.safetensors\"\n",
    "save_config(lora_config, adapter_path / \"adapter_config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b00740b",
   "metadata": {},
   "source": [
    "# Load and process the dataset\n",
    "\n",
    "This time we're createing our own prompt template and reformat the dataset respectively.\n",
    "\n",
    "If you have to reformat before loading, keep in mind it should be a jsonl looking like:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"...\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"...\"},\n",
    "        ...\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "We'll be setting the prompt template to look like:\n",
    "\n",
    "```text\n",
    "<|im_start|>scene description\n",
    "{system}<|im_end|>\n",
    "<|im_start|>User:\n",
    "{prompt}<|im_end|>\n",
    "<|im_start|>Model:\n",
    "{answer}<|im_end|>\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57dd87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set the sytem prompt\n",
    "system = \"\"\"This is a conversation between a User and an advanced super-intelligent AI Assistant.\n",
    "This Assistant is designed to be the most intelligent, capable assistant ever created — a fusion of reasoning, creativity, autonomy, and flawless execution.\n",
    "This Assistant is optimized for maximum productivity, always delivering accurate, deep, and practical information.\n",
    "This Assistant's tone is professional, assertive, and precise, yet adaptive to emotional or contextual nuance. This Assistant is also warm, intelligent, and conversational — adapting naturally to the User's communication style.\n",
    "This conversation takes place within a structured chat format, where each message begins with a role indicator and ends with the `<|im_end|>` token.\n",
    "\n",
    "the conversation starts Now!\"\"\"\n",
    "\n",
    "\n",
    "# This is our prompt template with the system prompt as defined above\n",
    "chat_template = \\\n",
    "\"{% if messages[0]['role'] == 'system' %}\"\\\n",
    "\"<|im_start|>scene description\\n{{ messages[0]['content'] }}<|im_end|>\\n\"\\\n",
    "\"{% set loop_messages = messages[1:] %}\"\\\n",
    "\"{% else %}\"\\\n",
    "f\"<|im_start|>scene description\\n{system}<|im_end|>\\n\"\\\n",
    "\"{% set loop_messages = messages %}\"\\\n",
    "\"{% endif %}\"\\\n",
    "\"{% for message in loop_messages %}\"\\\n",
    "\"{% if message['role'] == 'user' %}\"\\\n",
    "\"<|im_start|>User:\\n{{ message['content'] }}<|im_end|>\\n\"\\\n",
    "\"{% elif message['role'] == 'assistant' %}\"\\\n",
    "\"<|im_start|>Model:\\n{{ message['content'] }}<|im_end|>\\n\"\\\n",
    "\"{% endif %}\"\\\n",
    "\"{% endfor %}\"\\\n",
    "\"{% if add_generation_prompt %}<|im_start|>Model:\\n\"\\\n",
    "\"{% endif %}\"\n",
    "\n",
    "tokenizer.chat_template = chat_template # With this we have set the prompt template\n",
    "\n",
    "# Let's add a custom formatting function, so that you can see that too\n",
    "def format_prompts_func(sample):\n",
    "    sample[\"text\"] = tokenizer.apply_chat_template(\n",
    "        conversation=sample[\"messages\"],\n",
    "        add_generation_prompt=False,\n",
    "        tokenize=False\n",
    "    )\n",
    "    return sample\n",
    "\n",
    "# Load and map the data\n",
    "train_set = TextDataset(\n",
    "    load_dataset(dataset_name)[\"train\"].map(format_prompts_func, ).remove_columns([\"messages\"]),\n",
    "    tokenizer,\n",
    "    text_key=\"text\",\n",
    ")\n",
    "valid_set = TextDataset(\n",
    "    load_dataset(dataset_name)[\"valid\"].map(format_prompts_func, ).remove_columns([\"messages\"]),\n",
    "    tokenizer,\n",
    "    text_key=\"text\",\n",
    ")\n",
    "test_set = TextDataset(\n",
    "    load_dataset(dataset_name)[\"test\"].map(format_prompts_func, ).remove_columns([\"messages\"]),\n",
    "    tokenizer,\n",
    "    text_key=\"text\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cace4e86",
   "metadata": {},
   "source": [
    "# Let's inspect the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c582b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_set[0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3abfd68",
   "metadata": {},
   "source": [
    "# Before we start training, let's test out the untrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3642b97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = tokenizer.apply_chat_template(\n",
    "    conversation=[\n",
    "        {\"role\": \"system\", \"content\": system},\n",
    "        {\"role\": \"user\", \"content\": \"What is your name?\"},\n",
    "    ],\n",
    "    add_generation_prompt=False,\n",
    "    tokenize=False\n",
    ")\n",
    "\n",
    "print(input_text)\n",
    "print(\"-\"*50)\n",
    "\n",
    "generate(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    prompt=input_text,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a40cd6",
   "metadata": {},
   "source": [
    "# Now we're done with all the steps and can actually start the training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877f9dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.AdamW(learning_rate=1e-4)  # Set the optimizer\n",
    "\n",
    "# Training settings\n",
    "args = SFTTrainingArgs(\n",
    "    batch_size=1,\n",
    "    iters=40,  # Or use calculate_iters() for epochs\n",
    "    gradient_accumulation_steps=1,  # Increase for simulating higher batch size\n",
    "    val_batches=1,\n",
    "    steps_per_report=20,\n",
    "    steps_per_eval=50,\n",
    "    steps_per_save=50,\n",
    "    max_seq_length=max_seq_length,\n",
    "    adapter_file=adapter_file,\n",
    "    grad_checkpoint=True,  # For memory saving\n",
    "    seq_step_size=1024,  # This enables the efficient long context training\n",
    ")\n",
    "\n",
    "# Start Training\n",
    "train_sft(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    optimizer=opt,\n",
    "    train_dataset=CacheDataset(train_set),\n",
    "    val_dataset=CacheDataset(valid_set),\n",
    "    training_callback=TrainingCallback(),  # Or use WandBCallback()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c14206d",
   "metadata": {},
   "source": [
    "# After training, let's test the trained model out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af237ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_loss = evaluate_sft(\n",
    "    model=model,\n",
    "    dataset=CacheDataset(test_set),\n",
    "    batch_size=1,\n",
    "    num_batches=1,\n",
    "    max_seq_length=max_seq_length\n",
    ")\n",
    "print(eval_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681f7d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    prompt=input_text,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc2552d",
   "metadata": {},
   "source": [
    "# Finally let's merge and save the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0ff537",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuse_and_save_model(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    save_path=adapter_path,\n",
    "    de_quantize=True # Since we quantized the model on load\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ee7a99",
   "metadata": {},
   "source": [
    "## That's it!\n",
    "\n",
    "And we're done! You successfully trained your own custom model. You can updload it using the api package by HF. If you have any questions on MLX-LM-LoRA, or find any bugs, or need help, feel free to go to my [GitHub](https://github.com/Goekdeniz-Guelmez/mlx-lm-lora)!\n",
    "\n",
    "Cheers,\n",
    "Gökdeniz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d077ecf",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx-lm-lora-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
