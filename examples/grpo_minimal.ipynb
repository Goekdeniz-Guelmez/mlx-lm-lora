{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7ca9b44",
   "metadata": {},
   "source": [
    "# Train a custom reasoning model using MLX-LM-LoRA's DPO trainer\n",
    "\n",
    "I'm about to demonstrate the power of MLX-LM-LoRA through a RL example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee5f7bf",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install -U mlx-lm-lora ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac842fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The trainer and evaluations\n",
    "from mlx_lm_lora.trainer.grpo_trainer import GRPOTrainingArgs, train_grpo, evaluate_grpo\n",
    "\n",
    "# The Datasets\n",
    "from mlx_lm_lora.trainer.datasets import CacheDataset, GRPODataset\n",
    "\n",
    "# The reward functions\n",
    "from mlx_lm_lora.trainer.grpo_reward_functions import (\n",
    "    r1_accuracy_reward_func,\n",
    "    r1_int_reward_func,\n",
    "    r1_strict_format_reward_func,\n",
    "    r1_soft_format_reward_func,\n",
    "    r1_count_xml\n",
    ")\n",
    "\n",
    "# For loading/saving the model and calculating the steps\n",
    "from mlx_lm_lora.utils import from_pretrained, fuse_and_save_model, calculate_iters\n",
    "\n",
    "# For loading the dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Other needed stuff\n",
    "from mlx_lm.tuner.utils import print_trainable_parameters\n",
    "from mlx_lm.tuner.callbacks import TrainingCallback\n",
    "from mlx_lm.utils import save_config\n",
    "from pathlib import Path\n",
    "\n",
    "# The optimizer\n",
    "import mlx.optimizers as optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08959144",
   "metadata": {},
   "source": [
    "# Set the datase, model, and loading params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccaac3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen3-1.7B\"\n",
    "ref_model_name = \"Qwen/Qwen3-1.7B\"\n",
    "adapter_path = \"./tests\"\n",
    "dataset_name = \"mlx-community/Dolci-Think-RL-7B-2k\"\n",
    "\n",
    "max_seq_length = 512\n",
    "lora_config = { # LoRA adapter configuration\n",
    "    \"rank\": 8,  # Low-rank bottleneck size (Larger rank = smarter, but slower). Suggested 8, 16, 32, 64, 128\n",
    "    \"dropout\": 0.0,\n",
    "    \"scale\": 10.0, # Multiplier for how hard the LoRA update hits the base weights\n",
    "    \"use_dora\": False,\n",
    "    \"num_layers\": 8 # Use -1 for all layers\n",
    "}\n",
    "quantized_config={\n",
    "    \"bits\": 4, # Use 4 bit quantization. Suggested 4, 6, 8\n",
    "    \"group_size\": 64\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e11f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_model, _ = from_pretrained(\n",
    "    model=ref_model_name,\n",
    "    quantized_load=None, # Ref model shoudl be \"smarter\" then studend model\n",
    ")\n",
    "\n",
    "model, tokenizer = from_pretrained(\n",
    "    model=model_name,\n",
    "    lora_config=lora_config,\n",
    "    quantized_load=quantized_config,\n",
    ")\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1f3902",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_path = Path(adapter_path)\n",
    "adapter_path.mkdir(parents=True, exist_ok=True)\n",
    "adapter_file = adapter_path / \"adapters.safetensors\"\n",
    "save_config(lora_config, adapter_path / \"adapter_config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fddb12",
   "metadata": {},
   "source": [
    "# Load and process the dataset\n",
    "\n",
    "We don't have to format the Dataset the GRPODataset class will do that itself.\n",
    "\n",
    "If you have to reformat before loading, keep in mind it should be a jsonl looking like:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"prompt\": \"...\",\n",
    "    \"answer\": \"...\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcb9611",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = GRPODataset(\n",
    "    load_dataset(dataset_name)[\"train\"],\n",
    "    tokenizer,\n",
    "    prompt_key=\"prompt\",\n",
    "    answer_key=\"answer\",\n",
    "    system_key=\"system\",\n",
    "    type_key=\"type\"\n",
    ")\n",
    "valid_set = GRPODataset(\n",
    "    load_dataset(dataset_name)[\"valid\"],\n",
    "    tokenizer,\n",
    "    prompt_key=\"prompt\",\n",
    "    answer_key=\"answer\",\n",
    "    system_key=\"system\",\n",
    "    type_key=\"type\"\n",
    ")\n",
    "test_set = GRPODataset(\n",
    "    load_dataset(dataset_name)[\"test\"],\n",
    "    tokenizer,\n",
    "    prompt_key=\"prompt\",\n",
    "    answer_key=\"answer\",\n",
    "    system_key=\"system\",\n",
    "    type_key=\"type\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d0bf58",
   "metadata": {},
   "source": [
    "# Now we're done with all the steps and can actually start the training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6792253d",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.Muon(learning_rate=1e-4)  # Set the optimizer\n",
    "\n",
    "args = GRPOTrainingArgs(\n",
    "    batch_size=1,\n",
    "    iters=50,\n",
    "    gradient_accumulation_steps=1,\n",
    "    val_batches=1,\n",
    "    steps_per_report=1,\n",
    "    steps_per_eval=10,\n",
    "    steps_per_save=20,\n",
    "    max_seq_length=max_seq_length,\n",
    "    adapter_file=adapter_file,\n",
    "    grad_checkpoint=True,\n",
    "    group_size=1,\n",
    "    beta=0.01,\n",
    "    epsilon=0.1,\n",
    "    epsilon_high=0.3,\n",
    "    max_completion_length=max_seq_length//2,\n",
    "    reference_model_path=ref_model_name,\n",
    "    temperature=0.7,\n",
    "    grpo_loss_type=\"grpo\", # Chosse one: \"grpo\", \"bnpo\", \"dr_grpo\"\n",
    "    reward_weights=None,\n",
    "    importance_sampling_level=None # Choose one: \"token\", \"sequence\", None\n",
    ")\n",
    "\n",
    "train_grpo(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    ref_model=ref_model.freeze(),\n",
    "    args=args,\n",
    "    optimizer=opt,\n",
    "    train_dataset=CacheDataset(train_set),\n",
    "    val_dataset=CacheDataset(valid_set),\n",
    "    training_callback=TrainingCallback()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c94feb",
   "metadata": {},
   "source": [
    "# After training, let's test the trained model out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392a0d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, _, rewards = evaluate_grpo(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    ref_model=ref_model.freeze(),\n",
    "    dataset=CacheDataset(test_set),\n",
    "    batch_size=1,\n",
    "    num_batches=1,\n",
    "    max_seq_length=max_seq_length,\n",
    "    beta=0.01,\n",
    "    epsilon=0.1,\n",
    "    epsilon_high=0.3,\n",
    "    group_size=1,\n",
    "    max_tokens=max_seq_length//2,\n",
    "    temperature=0.7,\n",
    "    reward_funcs=[\n",
    "        r1_accuracy_reward_func,\n",
    "        r1_int_reward_func,\n",
    "        r1_strict_format_reward_func,\n",
    "        r1_soft_format_reward_func,\n",
    "        r1_count_xml\n",
    "    ],\n",
    "    grpo_loss_type=\"grpo\",\n",
    "    importance_sampling_level=None\n",
    ")\n",
    "print(loss)\n",
    "print(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ee0efb",
   "metadata": {},
   "source": [
    "# Finally let's merge and save the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ffe978",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuse_and_save_model(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    save_path=adapter_path,\n",
    "    de_quantize=True # Since we quantized the model on load\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe5c262",
   "metadata": {},
   "source": [
    "## That's it!\n",
    "\n",
    "And we're done! You successfully trained your own custom model. You can updload it using the api package by HF. If you have any questions on MLX-LM-LoRA, or find any bugs, or need help, feel free to go to my [GitHub](https://github.com/Goekdeniz-Guelmez/mlx-lm-lora)!\n",
    "\n",
    "Cheers,\n",
    "GÃ¶kdeniz"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "itsm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
