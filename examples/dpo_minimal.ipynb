{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7ca9b44",
   "metadata": {},
   "source": [
    "# Train a custom Chat model using MLX-LM-LoRA's DPO trainer\n",
    "\n",
    "I'm about to demonstrate the power of MLX-LM-LoRA through a preference optimization example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee5f7bf",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install -U mlx-lm-lora ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac842fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The trainer and evaluations\n",
    "from mlx_lm_lora.trainer.dpo_trainer import DPOTrainingArgs, evaluate_dpo, train_dpo\n",
    "\n",
    "# The Datasets\n",
    "from mlx_lm_lora.trainer.datasets import CacheDataset, PreferenceDataset\n",
    "\n",
    "# For loading/saving the model and calculating the steps\n",
    "from mlx_lm_lora.utils import from_pretrained, fuse_and_save_model, calculate_iters\n",
    "\n",
    "# For loading the dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Other needed stuff\n",
    "from mlx_lm.tuner.utils import print_trainable_parameters\n",
    "from mlx_lm.tuner.callbacks import TrainingCallback\n",
    "from mlx_lm.utils import save_config\n",
    "from pathlib import Path\n",
    "\n",
    "# The optimizer\n",
    "import mlx.optimizers as optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08959144",
   "metadata": {},
   "source": [
    "# Set the datase, model, and loading params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccaac3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen3-1.7B\"\n",
    "ref_model_name = \"Qwen/Qwen3-1.7B\"\n",
    "adapter_path = \"./tests\"\n",
    "dataset_name = \"mlx-community/Josiefied-Qwen3-dpo-v1-flat\"\n",
    "\n",
    "max_seq_length = 512\n",
    "lora_config = { # LoRA adapter configuration\n",
    "    \"rank\": 8,  # Low-rank bottleneck size (Larger rank = smarter, but slower). Suggested 8, 16, 32, 64, 128\n",
    "    \"dropout\": 0.0,\n",
    "    \"scale\": 10.0, # Multiplier for how hard the LoRA update hits the base weights\n",
    "    \"use_dora\": False,\n",
    "    \"num_layers\": 8 # Use -1 for all layers\n",
    "}\n",
    "quantized_config={\n",
    "    \"bits\": 4, # Use 4 bit quantization. Suggested 4, 6, 8\n",
    "    \"group_size\": 64\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e11f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_model, _ = from_pretrained(\n",
    "    model=ref_model_name,\n",
    "    quantized_load=None, # Ref model shoudl be \"smarter\" then studend model\n",
    ")\n",
    "\n",
    "model, tokenizer = from_pretrained(\n",
    "    model=model_name,\n",
    "    lora_config=lora_config,\n",
    "    quantized_load=quantized_config,\n",
    ")\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1f3902",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_path = Path(adapter_path)\n",
    "adapter_path.mkdir(parents=True, exist_ok=True)\n",
    "adapter_file = adapter_path / \"adapters.safetensors\"\n",
    "save_config(lora_config, adapter_path / \"adapter_config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fddb12",
   "metadata": {},
   "source": [
    "# Load and process the dataset\n",
    "\n",
    "We have to format the Dataset before feeding into the model in training.\n",
    "\n",
    "If you have to reformat before loading, keep in mind it should be a jsonl looking like:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"prompt\": \"...\",\n",
    "    \"chosen\": \"...\",\n",
    "    \"rejected\": \"...\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcb9611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format(sample):\n",
    "    prompt = sample[\"prompt\"]\n",
    "    chosen = sample[\"chosen\"]\n",
    "    rejected = sample[\"rejected\"]\n",
    "\n",
    "    sample[\"chosen\"] = tokenizer.apply_chat_template(\n",
    "        conversation=[\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "            {\"role\": \"assistant\", \"content\": chosen}\n",
    "        ],\n",
    "        add_generation_prompt=False,\n",
    "        enable_thinking=False,\n",
    "        tokenize=False\n",
    "    )\n",
    "\n",
    "    sample[\"rejected\"] = tokenizer.apply_chat_template(\n",
    "        conversation=[\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "            {\"role\": \"assistant\", \"content\": rejected}\n",
    "        ],\n",
    "        add_generation_prompt=False,\n",
    "        enable_thinking=False,\n",
    "        tokenize=False\n",
    "    )\n",
    "    return sample\n",
    "\n",
    "dataset = load_dataset(dataset_name)[\"train\"]\n",
    "train_dataset = dataset.select(range(0, 400)).map(format, ) # 400 samples for training\n",
    "valid_dataset = dataset.select(range(400, 460)).map(format, ) # 60 samples for validation\n",
    "test_dataset = dataset.select(range(460, 500)).map(format, ) # 40 samopes for testing at the end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59583587",
   "metadata": {},
   "source": [
    "# Let's inspect the loaded dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a829c18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"#\"*50 , \"Chosen\", \"#\"*100)\n",
    "print(train_dataset[0][\"chosen\"])\n",
    "print(\"#\"*50 , \"Rejected\", \"#\"*100)\n",
    "print(train_dataset[0][\"rejected\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9557eb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = PreferenceDataset(train_dataset, tokenizer, chosen_key=\"chosen\", rejected_key=\"rejected\")\n",
    "valid_set = PreferenceDataset(valid_dataset, tokenizer, chosen_key=\"chosen\", rejected_key=\"rejected\")\n",
    "test_set = PreferenceDataset(test_dataset, tokenizer, chosen_key=\"chosen\", rejected_key=\"rejected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d0bf58",
   "metadata": {},
   "source": [
    "# Now we're done with all the steps and can actually start the training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6792253d",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.Muon(learning_rate=1e-4)  # Set the optimizer\n",
    "\n",
    "args = DPOTrainingArgs(\n",
    "    batch_size=1,\n",
    "    iters=calculate_iters(train_set, batch_size=1, epochs=1),\n",
    "    gradient_accumulation_steps=1,\n",
    "    val_batches=1,\n",
    "    steps_per_report=1,\n",
    "    steps_per_eval=10,\n",
    "    steps_per_save=20,\n",
    "    max_seq_length=512,\n",
    "    adapter_file=adapter_file,\n",
    "    grad_checkpoint=True,\n",
    "    beta=0.1,\n",
    "    loss_type=\"sigmoid\", # Choose one: \"sigmoid\", \"hinge\", \"ipo\", \"dpop\"\n",
    "    delta=0.01,\n",
    "    reference_model_path=model_name\n",
    ")\n",
    "\n",
    "train_dpo(\n",
    "    model=model,\n",
    "    ref_model=model.freeze(),\n",
    "    args=args,\n",
    "    optimizer=opt,\n",
    "    train_dataset=CacheDataset(train_set),\n",
    "    val_dataset=CacheDataset(valid_set),\n",
    "    training_callback=TrainingCallback(),\n",
    "    loss_type=\"sigmoid\", # Choose one: \"sigmoid\", \"hinge\", \"ipo\", \"dpop\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392a0d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_dpo(\n",
    "    model=model,\n",
    "    ref_model=model.freeze(),\n",
    "    dataset=CacheDataset(test_set),\n",
    "    batch_size=1,\n",
    "    num_batches=1,\n",
    "    beta=0.1,\n",
    "    delta=0.01,\n",
    "    max_seq_length=512,\n",
    "    loss_type=\"sigmoid\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "itsm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
