{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Course: Training a Custom Tiny Qwen3 MoE LLM from Scratch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I welcome you to this hands-on course where we build and train a Large Language Model (LLM) entirely from scratch only on Apple Silicon using MLX-LM and my package MLX-LM-LoRA.\n",
    "\n",
    "This is not another ‚Äúfine-tune GPT‚Äù course ‚Äî we‚Äôre creating a custom MoE model architecture, pretraining it on web-scale data, finetuning it using supervised data, and optimizing it with preference learning. All of it runs locally on your Mac.\n",
    "\n",
    "---\n",
    "\n",
    "üì¶ Minimum Requirements\n",
    "\n",
    "To run this notebook locally, you‚Äôll need:\n",
    "- An Apple Silicon Mac (M1 Pro / M2 Max / M3, etc.)\n",
    "- Minimum 32 GB of unified memory (lower is also possible, but you'd have to change parameters to fit, Higher is better)\n",
    "- Python ‚â• 3.12\n",
    "\n",
    "---\n",
    "\n",
    "üìö Course Overview\n",
    "\n",
    "This course covers the entire LLM training stack:\n",
    "1.\tModel Design: Define a Tiny Qwen3 MoE-style model\n",
    "2.\tPretraining: Unsupervised training on `mlx-community/fineweb-200k`\n",
    "3.\tSupervised Finetuning (SFT): Using a subset of `digitalpipelines/wizard_vicuna_70k_uncensored`\n",
    "4.\tPreference Optimization: With `mlx-community/Human-Like-DPO`\n",
    "5.\tEvaluation + Inference: Test generations and evaluation\n",
    "\n",
    "---\n",
    "\n",
    "ü§ñ Model Architecture: Tiny Qwen3 MoE\n",
    "\n",
    "We‚Äôll build a custom minimal MoE version of the new Qwen3. The model uses:\n",
    "- Transformer decoder-only blocks\n",
    "- Sparse Mixture-of-Experts layers\n",
    "- Rotary embeddings (RoPE)\n",
    "\n",
    "We aim to keep it light enough to fully train on-device.\n",
    "\n",
    "---\n",
    "\n",
    "üß™ Pretraining Dataset: mlx-community/fineweb-200k\n",
    "\n",
    "We‚Äôll use a subset of the FineWeb dataset hosted on Hugging Face:\n",
    "mlx-community/fineweb-200k\n",
    "\n",
    "It is:\n",
    "- Cleaned, deduplicated web content\n",
    "- Ideal for unsupervised autoregressive training\n",
    "- Efficiently streamed on Apple Silicon\n",
    "\n",
    "---\n",
    "\n",
    "üßæ Prompt Templates\n",
    "\n",
    "We‚Äôll use a custom prompt template to go with Qwen3.\n",
    "For finetuning, we adopt the following ChatLM-style template:\n",
    "\n",
    "```text\n",
    "<|im_start|>system description\n",
    "This is a conversation between Josie, a helpfull AI assistant and a human user.<|im_end|>\n",
    "<|im_start|>user turn\n",
    "{prompt}<|im_end|>\n",
    "<|im_start|>assistant turn, name = 'Josie'\n",
    "{answer}<|im_end|>\n",
    "```\n",
    "\n",
    "This helps align with conversational-tuned models and enables easy preference optimization later.\n",
    "\n",
    "---\n",
    "\n",
    "üß† SFT Dataset: digitalpipelines/wizard_vicuna_70k_uncensored\n",
    "\n",
    "We‚Äôll use the Wizard Vicuna dataset, known for:\n",
    "- Rich instruction-following data\n",
    "- Clean conversational structure\n",
    "- Multi turn conversations\n",
    "- Aligned with assistant-style prompting\n",
    "\n",
    "This stage teaches the model how a conversation looks like.\n",
    "\n",
    "---\n",
    "\n",
    "‚ù§Ô∏è Preference Optimization: mlx-community/Human-Like-DPO\n",
    "\n",
    "To refine the model‚Äôs behavior further, We‚Äôll apply Monolithic Preference Optimization (ORPO) using:\n",
    "\n",
    "mlx-community/Human-Like-DPO\n",
    "\n",
    "This dataset includes:\n",
    "- Ranked preference pairs\n",
    "- Human-like instructions and completions\n",
    "- Fine-grained reward signal for better alignment\n",
    "\n",
    "---\n",
    "\n",
    "üß∞ Tools Used\n",
    "- MLX-LM: Apple-native LLM framework leveraging MLX runtime\n",
    "- MLX-LM-LoRA: My package for full-precision and LoRA training, supporting DPO, ORPO, GRPO and more\n",
    "- Hugging Face Datasets + Transformers for data loading and formatting\n",
    "\n",
    "---\n",
    "\n",
    "üèÅ By The End Of This Course‚Ä¶\n",
    "\n",
    "You‚Äôll have:\n",
    "- Designed and built a custom Qwen3 MoE model\n",
    "- Pretrained it on web-scale text\n",
    "- Supervised-finetuned it on instruction data\n",
    "- Aligned it using preference optimization\n",
    "- Deployed and evaluated it entirely on your Mac\n",
    "\n",
    "Let‚Äôs get started! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚öôÔ∏è Installation: Set Up the Environment\n",
    "\n",
    "Before we begin training, let‚Äôs install all necessary dependencies.\n",
    "\n",
    "We‚Äôll use:\n",
    "- `mlx-lm-lora` ‚Äì includes mlx-lm, datasets\n",
    "- `huggingface_hub` ‚Äì for uploading our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!pip install mlx-lm-lora huggingface_hub wandb hf_xet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üì¶ Imports and Setup\n",
    "\n",
    "Before building and training our Qwen3-style MoE model, we import everything needed:\n",
    "- Core Python utilities (os, Path, dataclass)\n",
    "- MLX-LM-LORA tools for defining models and saving configs\n",
    "- HF token/tokenizer setup\n",
    "- Model class and utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gokdenizgulmez/Desktop/mlx-lm-lora/.conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from mlx_lm_lora.trainer.datasets import CacheDataset, ORPODataset, TextDataset\n",
    "from mlx_lm_lora.trainer.orpo_trainer import ORPOTrainingArgs, train_orpo\n",
    "from mlx_lm_lora.trainer.sft_trainer import SFTTrainingArgs, train_sft\n",
    "\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import create_repo, HfApi\n",
    "\n",
    "from mlx_lm.utils import load_model, load_tokenizer, save_config, save_model\n",
    "from mlx_lm.models.qwen3_moe import Model, BaseModelArgs\n",
    "from mlx_lm.tuner.utils import get_total_parameters\n",
    "from mlx_lm.tuner.callbacks import TrainingCallback\n",
    "from mlx_lm import generate\n",
    "\n",
    "import mlx.optimizers as optim\n",
    "from mlx_optimizers import Muon\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "import math\n",
    "import os\n",
    "\n",
    "def calculate_iters(train_set, batch_size, epochs) -> int:\n",
    "    num_samples = len(train_set)\n",
    "    batches_per_epoch = math.ceil(num_samples / batch_size)\n",
    "    iters = epochs * batches_per_epoch\n",
    "    print(f\"[INFO] Calculated {iters} iterations from {epochs} epochs (dataset size: {num_samples}, batch size: {batch_size})\")\n",
    "    return iters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìå Setup: Define HF Token and Model Metadata\n",
    "\n",
    "Before training or uploading, we set up:\n",
    "- Your Hugging Face token for authentication\n",
    "- The model name (used for saving)\n",
    "- The user or organization name\n",
    "- The local path where everything will be stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token = \"\" # <-- Add you HF Token here\n",
    "\n",
    "new_model_name = \"qwen3_tiny_moe\"\n",
    "user_name = \"mlx-community\"\n",
    "author = \"G√∂kdeniz G√ºlmez\"\n",
    "\n",
    "folder = \"/Users/gokdenizgulmez/Desktop/mlx-lm-lora/examples/\"\n",
    "\n",
    "pretraining_dataset_name = \"mlx-community/fineweb-200k\"\n",
    "pretraining_dataset_samples = 2000\n",
    "\n",
    "finetuning_dataset_name = \"digitalpipelines/wizard_vicuna_70k_uncensored\"\n",
    "finetuning_dataset_samples = 1000\n",
    "\n",
    "preference_dataset_name = \"mlx-community/Human-Like-DPO\"\n",
    "preference_dataset_samples = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìÅ Prepare Target Directory for Model & Tokenizer\n",
    "\n",
    "We define the full target path for saving the model and tokenizer.\n",
    "\n",
    "If it doesn‚Äôt already exist, we:\n",
    "- Clone the custom tokenizer repo from Hugging Face\n",
    "- Place it inside the model folder\n",
    "\n",
    "Otherwise, we skip cloning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer already exists at: /Users/gokdenizgulmez/Desktop/mlx-lm-lora/examples/qwen3_tiny_moe\n"
     ]
    }
   ],
   "source": [
    "target_dir = os.path.join(folder, new_model_name)\n",
    "if not os.path.exists(target_dir):\n",
    "    !git clone https://huggingface.co/Goekdeniz-Guelmez/qwen3_tokenizer \"{target_dir}\"\n",
    "else:\n",
    "    print(f\"Tokenizer already exists at: {target_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß± Define Model Configuration\n",
    "\n",
    "We subclass the BaseModelArgs to create a tiny Qwen3 MoE architecture:\n",
    "- Shallow network with only 2 layers\n",
    "- Small hidden size (128) for fast local training\n",
    "- 4 experts, 1 active per token (simple MoE routing)\n",
    "- Tokenizer and embedding sizes that match Qwen3\n",
    "\n",
    "This keeps the model small enough to:\n",
    "- Fit on local Apple Silicon\n",
    "- Still support pretraining and ORPO finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class NewModelArgs(BaseModelArgs):\n",
    "    mlp_only_layers: list[int] = field(default_factory=list)  # Default: no MLP-only layers\n",
    "\n",
    "    model_type: str = \"qwen3_moe\"\n",
    "    \n",
    "    hidden_size: int = 128                   # Tiny, but enough for basic functionality\n",
    "    num_hidden_layers: int = 2               # Very shallow\n",
    "    intermediate_size: int = 256             # Typically 2x hidden size\n",
    "    moe_intermediate_size: int = 256         # Same as above, or slightly larger\n",
    "    num_attention_heads: int = 2             # Must divide hidden_size\n",
    "    num_key_value_heads: int = 1             # Often set to 1 for tiny models\n",
    "    head_dim: int = field(init=False)        # Will be computed post-init\n",
    "    num_experts: int = 4                     # Small MoE with minimal experts\n",
    "    num_experts_per_tok: int = 1             # One expert per token\n",
    "    decoder_sparse_step: int = 1             # Typical setting\n",
    "\n",
    "    rms_norm_eps: float = 1e-6               # Standard value\n",
    "    vocab_size: int = 151936                 # Matches Qwen3 tokenizer size\n",
    "    rope_theta: float = 1000.0               # Qwen3 uses 1e3\n",
    "\n",
    "    tie_word_embeddings: bool = True         # Save params, good default\n",
    "    max_position_embeddings: int = 1028      # Common default\n",
    "    norm_topk_prob: bool = True              # MoE-specific regularization trick\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.head_dim = self.hidden_size // self.num_attention_heads  # Auto-calculated\n",
    "\n",
    "args = NewModelArgs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß™ Initialize the Model\n",
    "\n",
    "We instantiate the model using the configuration from the previous cell. This builds the full architecture in memory, ready for pretraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîç Inspect the Model\n",
    "\n",
    "We print:\n",
    "- The full architecture and parameter count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (model): Qwen3MoeModel(\n",
      "    (embed_tokens): Embedding(151936, 128)\n",
      "    (layers.0): Qwen3MoeDecoderLayer(\n",
      "      (self_attn): Attention(\n",
      "        (q_proj): Linear(input_dims=128, output_dims=128, bias=False)\n",
      "        (k_proj): Linear(input_dims=128, output_dims=64, bias=False)\n",
      "        (v_proj): Linear(input_dims=128, output_dims=64, bias=False)\n",
      "        (o_proj): Linear(input_dims=128, output_dims=128, bias=False)\n",
      "        (q_norm): RMSNorm(64, eps=1e-06)\n",
      "        (k_norm): RMSNorm(64, eps=1e-06)\n",
      "        (rope): RoPE(64, traditional=False)\n",
      "      )\n",
      "      (input_layernorm): RMSNorm(128, eps=1e-06)\n",
      "      (post_attention_layernorm): RMSNorm(128, eps=1e-06)\n",
      "      (mlp): Qwen3MoeSparseMoeBlock(\n",
      "        (gate): Linear(input_dims=128, output_dims=4, bias=False)\n",
      "        (switch_mlp): SwitchGLU(\n",
      "          (gate_proj): SwitchLinear()\n",
      "          (up_proj): SwitchLinear()\n",
      "          (down_proj): SwitchLinear()\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (layers.1): Qwen3MoeDecoderLayer(\n",
      "      (self_attn): Attention(\n",
      "        (q_proj): Linear(input_dims=128, output_dims=128, bias=False)\n",
      "        (k_proj): Linear(input_dims=128, output_dims=64, bias=False)\n",
      "        (v_proj): Linear(input_dims=128, output_dims=64, bias=False)\n",
      "        (o_proj): Linear(input_dims=128, output_dims=128, bias=False)\n",
      "        (q_norm): RMSNorm(64, eps=1e-06)\n",
      "        (k_norm): RMSNorm(64, eps=1e-06)\n",
      "        (rope): RoPE(64, traditional=False)\n",
      "      )\n",
      "      (input_layernorm): RMSNorm(128, eps=1e-06)\n",
      "      (post_attention_layernorm): RMSNorm(128, eps=1e-06)\n",
      "      (mlp): Qwen3MoeSparseMoeBlock(\n",
      "        (gate): Linear(input_dims=128, output_dims=4, bias=False)\n",
      "        (switch_mlp): SwitchGLU(\n",
      "          (gate_proj): SwitchLinear()\n",
      "          (up_proj): SwitchLinear()\n",
      "          (down_proj): SwitchLinear()\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): RMSNorm(128, eps=1e-06)\n",
      "  )\n",
      "  (lm_head): Linear(input_dims=128, output_dims=151936, bias=False)\n",
      ")\n",
      "39.000M total parameters.\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "print(f\"{int(get_total_parameters(model) / 1e6):.3f}M total parameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üíæ Save Model & Config to Disk\n",
    "\n",
    "We store:\n",
    "- The full args configuration as config.json\n",
    "- The initial (untrained) model weights using save_model(...)\n",
    "\n",
    "This gives us a checkpointable starting point for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_config(vars(args), f\"{target_dir}/config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = load_tokenizer(Path(target_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------------------------------------- Start Pretraning -------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚öôÔ∏è Initialize Optimizer (Muon)\n",
    "\n",
    "We initialize our optimizer ‚Äî in this case, Muon, a performant optimizer for LLM pretraining. We use a small learning rate to ensure stable convergence during early training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretraining_opt = Muon(learning_rate=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìÑ Load Pretraining Dataset\n",
    "\n",
    "Now we load the unsupervised pretraining dataset. We‚Äôre using a cleaned, deduplicated web corpus called FineWeb, hosted on Hugging Face.\n",
    "\n",
    "We also support subsampling the dataset (e.g., for testing), and we split it into training and validation sets for tracking generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretraining_dataset = load_dataset(pretraining_dataset_name)[\"train\"]\n",
    "\n",
    "if pretraining_dataset_samples is not None:\n",
    "    pretraining_dataset = pretraining_dataset.select(range(pretraining_dataset_samples))\n",
    "\n",
    "pretraining_train_dataset, pretraining_valid_dataset = pretraining_dataset.train_test_split(test_size=0.01, seed=42).values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üì¶ Format Dataset for Tokenized Training\n",
    "\n",
    "We wrap the raw dataset into TextDataset objects, which tokenize the samples and prepare them for training. Each sample is a single long-form text entry, streamed and tokenized efficiently on-device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretraining_train_set = TextDataset(pretraining_train_dataset, tokenizer, text_key='text')\n",
    "pretraining_valid_set = TextDataset(pretraining_valid_dataset, tokenizer, text_key='text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß™ Test Model Before Training\n",
    "\n",
    "Let‚Äôs try generating a basic output with our untrained model. This helps verify that the architecture is wired up correctly before starting pretraining.\n",
    "\n",
    "Of course, the output will be gibberish at this point ‚Äî the model hasn‚Äôt learned anything yet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ÔøΩ androidxÔøΩ arschÔøΩ Corvette_Form)(\"ÔøΩ ÔøΩ ÔøΩ ÔøΩ ÔøΩ ÔøΩ ÔøΩ ÔøΩ ÔøΩ ÔøΩ ÔøΩ ÔøΩ ÔøΩ ÔøΩ {ÔøΩ≈†ÔøΩ≈†#for-shapedivec)(\".RowHeadersTransportÔøΩ retorno√•¬∞ƒ≥√•¬•¬≥√•¬∏ƒÆ√¶ƒæƒΩ√©ƒ¢ƒº√®¬øƒ©.Configuration)(\"ÔøΩ BTNÔøΩ MechMigration_metrics=@√•¬±ƒ∑√•ƒÆ¬∫√´ƒßƒ∑tÔøΩ‚Äû¬±√§¬Ω¬∞=(\\'√•¬∞ƒ≥√ßƒ™¬∑√Ø¬≥ƒ¥√£ƒ•ƒ•√£ƒ•ƒ™ÔøΩ vÔøΩ∆í¬©jit√£ƒ§¬ß√∞ÔøΩ\\x81ƒØÔøΩ\\xadÔøΩ dÔøΩ∆í¬©cpromptÔøΩ ConsultantsÔøΩ‚Äû¬±dÔøΩ‚Äû¬±rStoresÔøΩ subscribeÔøΩ caracterÔøΩ∆íÔøΩ∆ísticasÔøΩ ves√•¬°ÔøΩ‚Ç¨√•¬∞ƒ∂√Ø¬≥ƒ¥rÔøΩ∆í¬§gocoÔøΩ‚Äûƒ∂.Param_updÔøΩ PKÔøΩ‚ÄòƒØÔøΩ\\x90¬ΩÔøΩ‚Äòƒ£ÔøΩ\\x90¬ª√ßƒ§ÔøΩ\\x81√£ƒ£¬®√£ƒ£ƒπ√£ƒ£¬¶√£ƒ§ƒ§ÔøΩ smallÔøΩ sucessÔøΩ explosiveÔøΩ ¬∏ƒ¶ÔøΩ ¬∏¬∏ÔøΩ ¬∏ƒµÔøΩ ¬∏¬™ÔøΩ ¬∏¬°ÔøΩ ¬∏ƒºÔøΩ ¬∏¬±ÔøΩ ¬∏ƒ∑ÔøΩ ¬∏¬¥)did.skÔøΩ muj...]ÔøΩ≈†ÔøΩ≈†ÔøΩ ÔøΩ\\x90¬¥ÔøΩ\\x90¬æÔøΩ‚ÄòƒßÔøΩ\\x90¬æÔøΩ\\x90¬¥√¶ƒ¶ƒ±√•ƒ≤ƒ≥.YesNoÔøΩÀú¬ßÔøΩ‚Ñ¢ƒ£ÔøΩÀú¬©√¶ƒ®ƒ∞ONSZERO√•¬¶ƒ©√®ƒ£ƒ∂ÔøΩ‚Äòƒ¢ÔøΩ‚Äòƒ•ÔøΩ\\x90¬≥ÔøΩ √™¬µƒ´√¨ÔøΩ‚Ç¨¬•√¢ƒª¬±ÔøΩ ¬∏ƒ∑ÔøΩ ¬∏¬±ÔøΩ ¬∏ƒ∂ÔøΩ ¬∏¬™ÔøΩ ¬∏¬¥ÔøΩ ¬∏ƒªÔøΩ ¬πƒ•ÔøΩ ¬∏ƒ™ÔøΩ manifests√©ƒ∂¬µÔøΩ‚Äîƒ∑ÔøΩ‚ÄîƒµÔøΩ‚Äîƒ∑ÔøΩ‚Äî¬™√•¬§ƒ´√£ƒ§ƒ±√£ƒ£¬£√£ƒ£¬¶NineVERTulenceÔøΩ‚Ä∞redirect√®ƒ£ƒ®√•ƒ≤¬¨√•ƒ≤ƒÆ√ßƒΩÔøΩ\\x81√¢ƒ¨¬∑/posts√∞ƒøƒ≤¬§IDADE√°¬¥¬∑√§¬∏ƒ¢√ßƒ§¬πÔøΩ‚Äîƒ∑ÔøΩ‚ÄîƒµÔøΩ‚Äîƒ∑ÔøΩ‚Äî¬™ÔøΩ METHÔøΩ menstr.YesNo_super√¢ƒµƒπÔøΩ ÔøΩ\\xadƒ∑ƒ¶rawerÔøΩ deepen√¨ƒ∏ƒ¶wiÔøΩ‚Ä¶ƒΩcie*vÔøΩ samplingitianÔøΩÀú¬ßÔøΩ‚Ñ¢ƒ£ÔøΩÀú¬©ÔøΩ ÔøΩ ¬∏ÔøΩ‚Ç¨ÔøΩÀú¬ßÔøΩ‚Ñ¢ƒ£ÔøΩÀú¬©Alter√¢ƒ¨¬∑partsdepend_wrapper-unusedÔøΩÀú¬ßÔøΩ‚Ñ¢ƒ™ÔøΩ‚Ñ¢ƒ¨√©ÔøΩ‚Ç¨ÔøΩ\\xadÔøΩ INVÔøΩ patientsÔøΩ‚Ä∞Add√•¬¨¬∑√•¬§ƒ∏√•¬Ω¬¢√¶ƒ©¬¶ÔøΩÀú¬ßÔøΩ‚Ñ¢ƒ£ÔøΩÀú¬©Mountonia================================================ÔøΩ WoodÔøΩ Starrunsafeaneous¬≥¬ªÔøΩÀú¬ßÔøΩÀú¬ÆÔøΩÀú¬™ÔøΩ‚Ñ¢ƒ¶ÔøΩÀú¬ßÔøΩ‚Ñ¢ƒ£HayÔøΩ skipped--)ÔøΩ≈†.putIntÔøΩ gaussianvaluesÔøΩ Motorcycle√¶ƒ•ƒß/Sub_EXISTS√•ƒß¬≥√§¬∫ƒ∞√§¬∏ƒØ√ß¬®¬≥√•¬Æƒº√ßƒæƒ£√•ƒß¬¨√•¬Æƒ´ÔøΩ Unauthorized.AutoScaleDimensionsÔøΩ\\x90¬µÔøΩ\\x90¬Ω(--ÔøΩ explanatoryPROGRAMÔøΩ BoyÔøΩ SerenaÔøΩ MICROWinter(COLORreation√¶¬°ÔøΩ‚Äö.getMaxÔøΩ PortÔøΩ ¬πƒ¢ÔøΩ ¬∏ÔøΩ\\x81√£ÔøΩ\\xad¬°√©ƒ∫¬µ√©ƒ∫¬µÔøΩ ÔøΩ\\x90¬øÔøΩ\\x90¬ªÔøΩ\\x90¬∞ÔøΩ‚Äòƒ£ÔøΩ‚Äòƒ§√®¬µƒ¶√¶ƒæ¬¨√§¬∏¬ª√§¬πƒ´√•¬∫ƒ∂√•ƒ±ƒ¨√¶ƒπ¬∂ÔøΩ loi<HTMLInputElement√¶ƒπ¬•√¶ƒæ¬¨√§¬∫¬∫onomicÔøΩ cazzoÔøΩ globe√©ƒ¢ƒ£√§¬∏ƒ¨ÔøΩ mxÔøΩ addAction_initializer√∞ƒøƒ≤¬§ÔøΩ noct(verticesTheoryÔøΩ amigosÔøΩ solidarity√¶ƒπ¬∞ÔøΩ\\x90¬ªÔøΩ‚ÄòÔøΩ\\xad√¶ÔøΩ\\x81ƒº√¶ƒΩ¬∏?</√¶ÔøΩ\\xad¬ø√©¬≤¬Ø√§¬Ω¬∞_doubleÔøΩÀú¬ØÔøΩ‚Ñ¢ƒ™ÔøΩ‚Ñ¢ƒ¶ÔøΩ‚Ä∞ÔøΩ‚Ä∞ÔøΩ‚Ä∞ÔøΩ‚Ä∞ÔøΩ‚Ä∞ÔøΩ‚Ä∞ÔøΩ‚Ä∞ÔøΩ‚Ä∞ÔøΩ‚Ä∞ÔøΩ‚Ä∞ÔøΩ‚Ä∞ÔøΩ‚Ä∞ÔøΩ‚Ä∞ÔøΩ‚Ä∞ÔøΩ‚Ä∞ÔøΩ Lamb√•¬¶ƒ©√®ƒ£ƒ∂√¢ƒ∑ƒ±.wayÔøΩ gdyÔøΩ‚Ä¶¬ºsuffix√¨¬¢ƒßÔøΩ Wir√¶ƒøƒ¢√•¬Æ¬≥ÔøΩ readilyÔøΩ\\xadƒ¨ƒ™√¶ƒæƒ¢√§¬∏¬ª√®¬¶ƒ£-------------ÔøΩ ÔøΩ\\x90¬≤ÔøΩ\\x90¬µÔøΩ‚Äòƒ£ÔøΩ‚ÄòƒÆÔøΩ RequestOptionsÔøΩ mollÔøΩ twinsÔøΩ textStyle_once.onCreateÔøΩ inquiryÔøΩ HamptoncephÔøΩ Ingram.getBeanÔøΩ ConfederÔøΩ Shiv√®ƒ±ƒ±√∞ÔøΩ\\x81ƒ≥ƒßtyardinton_initializerfully√¶¬πƒ∏√•¬∑ÔøΩ‚Ç¨√™ƒ§ÔøΩ indictÔøΩ tangent√•ƒ±ƒÆ√ßƒæ¬º√´ÔøΩ\\xad¬ªÔøΩ ÔøΩ‚Äîƒ∑ÔøΩ‚ÄîÔøΩ‚Ç¨ÔøΩ‚Äî¬™getOptionapoÔøΩ achievedÔøΩ WIDTH√∞ÔøΩ\\x81ƒ¶¬∏√£ƒ¢ƒ§ÔøΩ≈†ÔøΩ≈†ÔøΩ∆í¬°b:\\\\/\\\\/ÔøΩ ph√°¬∫¬ßnÔøΩ WorseÔøΩ faÔøΩ Mercy√•¬Ωƒµ√§¬∏ÔøΩ∆í√©ƒ¢ƒ£√•ƒ©¬∫_SCROLL√®¬Ø¬¥√ßƒºƒ¶√®¬Øƒø√•¬∏ƒ§√•ƒæ¬∫√®¬ßƒ¶√¶¬®¬°zeigtÔøΩ ÔøΩ\\xadƒ∑ƒ¶√¨ƒºƒ∂ÔøΩ sektÔøΩ∆í¬∂rÔøΩ caucuscoralytics'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    prompt=\"Hello\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Begin Pretraining\n",
    "\n",
    "Now we start pretraining the model using the unsupervised dataset.\n",
    "\n",
    "Key hyperparameters:\n",
    "- Batch size and number of epochs\n",
    "- Number of training iterations\n",
    "- Evaluation and checkpoint saving intervals\n",
    "- Gradient checkpointing (to reduce memory usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training..., iters: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1: Val loss 12.094, Val took 0.330s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  2.03s/it, loss=12.109, it/s=61.965]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iter 1: loss 12.109, lr 1.000e-05, it/s 61.965, tok/s 2221.448, trained_tok 3585, peak_mem 16.992GB\n",
      "Saved final weights to /Users/gokdenizgulmez/Desktop/mlx-lm-lora/examples/qwen3_tiny_moe/pretrain.safetensors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "epochs = 16\n",
    "\n",
    "train_sft(\n",
    "    model=model,\n",
    "    args=SFTTrainingArgs(\n",
    "        batch_size=batch_size,\n",
    "        iters=calculate_iters(train_set=pretraining_train_set, batch_size=batch_size, epochs=epochs),\n",
    "        val_batches=1,\n",
    "        steps_per_report=100,\n",
    "        steps_per_eval=1000,\n",
    "        steps_per_save=1000,\n",
    "        max_seq_length=model.args.max_position_embeddings,\n",
    "        grad_checkpoint=True,\n",
    "        adapter_file=Path(target_dir) / \"pretrain.safetensors\",\n",
    "    ),\n",
    "    optimizer=pretraining_opt,\n",
    "    train_dataset=CacheDataset(pretraining_train_set),\n",
    "    val_dataset=CacheDataset(pretraining_valid_set),\n",
    "    training_callback=TrainingCallback()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚úÖ Generate After Pretraining\n",
    "\n",
    "Once training is done, we generate another output using the now pretrained model. This should show noticeable improvements over the gibberish from earlier.\n",
    "\n",
    "From here, we‚Äôll move on to supervised finetuning using instruction-following data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    prompt=\"Hello, \"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------------------------------------- Start Fine-Tuning -------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üì• Load Pretrained Model & Define Sequence Length\n",
    "\n",
    "We begin by loading the pretrained model weights from the pretraining stage. This gives us a language model with a solid foundation in general web-text patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üí¨ Define Prompt Format\n",
    "\n",
    "To make our model follow instructions in a conversational way, we define a structured prompt format based on ChatML-style tokens used by Qwen3:\n",
    "- Each turn in the conversation is clearly labeled (system, user, assistant)\n",
    "- We also include special formatting tokens and the assistant name ‚ÄúJosie‚Äù\n",
    "- The full conversation is terminated with an <eos> token after each turn\n",
    "\n",
    "This format is essential for making the model align well with multi-turn dialogue patterns.\n",
    "\n",
    "# üìö Load and Format Finetuning Dataset\n",
    "\n",
    "We load the supervised finetuning dataset ‚Äî Wizard Vicuna ‚Äî which contains rich, multi-turn conversations.\n",
    "\n",
    "We then apply a formatting function that:\n",
    "- Converts each conversation into a full prompt using the template\n",
    "- Adds EOS tokens after every message\n",
    "- Builds up a structured context that can be fed directly into the model\n",
    "\n",
    "Finally, we split the dataset into training and validation sets for model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"This is a conversation between Josie, a helpfull AI assistant and a human user.\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "full_prompt_format = \"\"\"<|im_start|>system description\n",
    "{}<|im_end|>\n",
    "<|im_start|>user turn\n",
    "{}<|im_end|>\n",
    "<|im_start|>assistant turn, name = 'Josie'\n",
    "\"\"\"\n",
    "\n",
    "system_turn = \"\"\"<|im_start|>system description\n",
    "{}\"\"\"\n",
    "\n",
    "user_turn = \"\"\"\n",
    "<|im_start|>user turn\n",
    "{}\"\"\"\n",
    "\n",
    "assistant_turn = \"\"\"\n",
    "<|im_start|>assistant turn, name = 'Josie'\n",
    "{}\"\"\"\n",
    "\n",
    "def format_prompts_func(sample):\n",
    "    this_conversation = sample[\"conversations\"]\n",
    "\n",
    "    if isinstance(this_conversation, list):\n",
    "        conversation = system_turn.format(system_prompt + EOS_TOKEN)\n",
    "        for turn in this_conversation:\n",
    "            if turn[\"from\"] == \"human\":\n",
    "                conversation += user_turn.format(turn['value'] + EOS_TOKEN)\n",
    "            elif turn[\"from\"] == \"gpt\":\n",
    "                conversation += assistant_turn.format(turn['value'] + EOS_TOKEN)\n",
    "\n",
    "    sample[\"text\"] = conversation\n",
    "    return sample\n",
    "\n",
    "finetuning_dataset = load_dataset(finetuning_dataset_name)[\"train\"]\n",
    "\n",
    "if finetuning_dataset_samples is not None:\n",
    "    finetuning_dataset = finetuning_dataset.select(range(finetuning_dataset_samples))\n",
    "\n",
    "finetuning_dataset = finetuning_dataset.map(format_prompts_func,)\n",
    "finetuning_train_dataset, finetuning_valid_dataset = finetuning_dataset.train_test_split(test_size=0.01, seed=42).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuning_train_set = TextDataset(finetuning_train_dataset, tokenizer, text_key='text')\n",
    "finetuning_valid_set = TextDataset(finetuning_valid_dataset, tokenizer, text_key='text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚öôÔ∏è Initialize Finetuning Optimizer\n",
    "\n",
    "We use the standard AdamW optimizer for supervised finetuning, with a learning rate tuned for stability and generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuning_opt = optim.AdamW(learning_rate=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ Generate Before Finetuning\n",
    "\n",
    "Just like with pretraining, we generate an output from the model before starting finetuning. This gives us a baseline for comparison and ensures that formatting is working correctly.\n",
    "\n",
    "At this stage, the model understands language but isn‚Äôt yet great at instruction-following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    prompt=full_prompt_format.format(system_prompt + EOS_TOKEN, \"Hello, how are you?\" + EOS_TOKEN)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Train with Supervised Instruction Data\n",
    "\n",
    "Now we begin the finetuning process!\n",
    "\n",
    "Using train_sft, we fine-tune the model on instructional conversations using:\n",
    "- A smaller batch size\n",
    "- Shorter training schedule (6 epochs)\n",
    "- Frequent evaluation and saving\n",
    "\n",
    "This helps the model learn how to answer direct questions, follow instructions, and respond conversationally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "epochs = 6\n",
    "\n",
    "train_sft(\n",
    "    model=model,\n",
    "    args=SFTTrainingArgs(\n",
    "        batch_size=batch_size,\n",
    "        iters=calculate_iters(train_set=finetuning_train_set, batch_size=batch_size, epochs=epochs),\n",
    "        val_batches=1,\n",
    "        steps_per_report=50,\n",
    "        steps_per_eval=100,\n",
    "        steps_per_save=100,\n",
    "        max_seq_length=512,\n",
    "        grad_checkpoint=True,\n",
    "        adapter_file=Path(target_dir) / \"finetune.safetensors\",\n",
    "    ),\n",
    "    optimizer=finetuning_opt,\n",
    "    train_dataset=CacheDataset(finetuning_train_set),\n",
    "    val_dataset=CacheDataset(finetuning_valid_set),\n",
    "    training_callback=TrainingCallback()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚úÖ Generate After Finetuning\n",
    "\n",
    "Finally, we generate again after training to see how the model improved. You should now notice:\n",
    "- More human and aligned responses\n",
    "- Better task-following ability\n",
    "- Less randomness compared to pretraining-only output\n",
    "\n",
    "From here, we‚Äôre ready to further refine the model with preference optimization, teaching it not just what to say ‚Äî but how to say it better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    prompt=full_prompt_format.format(system_prompt + EOS_TOKEN, \"Hello, how are you?\" + EOS_TOKEN)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------------------------------------- Optimize for Preference -------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üóÇÔ∏è Load and Format Preference Dataset\n",
    "\n",
    "We load the preference dataset ‚Äî such as one based on human or synthetic rankings (e.g. OpenAssistant, Anthropic HH-RLHF, etc.).\n",
    "\n",
    "Each entry is transformed into:\n",
    "- A chosen version: the preferred assistant response\n",
    "- A rejected version: the less helpful or lower-quality completion\n",
    "\n",
    "This dual-format is essential for contrastive training using ORPO.\n",
    "\n",
    "We split the dataset into training and validation subsets to monitor generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"This is a conversation between Josie, a helpfull AI assistant and a human user.\"\"\"\n",
    "\n",
    "def format_prompts_func(sample):\n",
    "    prompt = sample[\"prompt\"]\n",
    "    chosen = sample[\"chosen\"]\n",
    "    rejected = sample[\"rejected\"]\n",
    "\n",
    "    chosen_conversation = full_prompt_format.format(system_prompt, prompt, chosen)\n",
    "    rejected_conversation = full_prompt_format.format(system_prompt, prompt, rejected)\n",
    "\n",
    "    sample[\"rejected\"] = rejected_conversation\n",
    "    sample[\"chosen\"] = chosen_conversation\n",
    "    return sample\n",
    "\n",
    "preference_dataset = load_dataset(preference_dataset_name)[\"train\"]\n",
    "\n",
    "if preference_dataset_samples is not None:\n",
    "    preference_dataset = preference_dataset.select(range(preference_dataset_samples))\n",
    "\n",
    "preference_dataset = preference_dataset.map(format_prompts_func,)\n",
    "preference_train_dataset, preference_valid_dataset = preference_dataset.train_test_split(test_size=0.01, seed=42).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "preference_train_set = ORPODataset(preference_train_dataset, tokenizer)\n",
    "preference_valid_set = ORPODataset(preference_valid_dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚öôÔ∏è Set Up Preference Optimizer\n",
    "\n",
    "We initialize the optimizer ‚Äî again using AdamW with a modest learning rate, suitable for low-noise fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "preference_opt = optim.AdamW(learning_rate=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ Generate Before Preference Optimization\n",
    "\n",
    "Before we start optimizing with ORPO, we generate an example using the current finetuned model.\n",
    "\n",
    "This helps us evaluate how the model behaves before it has been trained to prefer better completions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    prompt=full_prompt_format.format(system_prompt + EOS_TOKEN, \"Hello, how are you?\" + EOS_TOKEN)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Train with ORPO: Reward-Based Fine-Tuning\n",
    "\n",
    "Now we train the model using train_orpo, a contrastive objective where the model is encouraged to:\n",
    "- Increase the likelihood of the chosen response\n",
    "- Decrease the likelihood of the rejected one\n",
    "\n",
    "We use:\n",
    "- Small batch size and modest epoch count\n",
    "- Frequent evaluation and saving\n",
    "- Hyperparameters like beta (regularization strength) and reward_scaling to balance reward magnitudes\n",
    "\n",
    "Importantly, we save only the adapter weights in this step, making the fine-tuning modular and efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "epochs = 4\n",
    "\n",
    "train_orpo(\n",
    "    model=model,\n",
    "    args=ORPOTrainingArgs(\n",
    "        batch_size=batch_size,\n",
    "        iters=calculate_iters(train_set=preference_train_set, batch_size=batch_size, epochs=epochs),\n",
    "        val_batches=1,\n",
    "        steps_per_report=20,\n",
    "        steps_per_eval=50,\n",
    "        steps_per_save=50,\n",
    "        max_seq_length=512,\n",
    "        grad_checkpoint=True,\n",
    "        beta=0.1,\n",
    "        reward_scaling=0.6,\n",
    "        adapter_file=Path(target_dir) / \"preference.safetensors\",\n",
    "    ),\n",
    "    optimizer=preference_opt,\n",
    "    train_dataset=CacheDataset(preference_train_set),\n",
    "    val_dataset=CacheDataset(preference_valid_set),\n",
    "    training_callback=TrainingCallback()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚úÖ Generate After Preference Optimization\n",
    "\n",
    "After training, we generate again to compare results. The model should now:\n",
    "- Prefer clearer, more helpful responses\n",
    "- Avoid overly long or repetitive completions\n",
    "- Show improved alignment with user intent\n",
    "\n",
    "This final stage wraps up the instruction-tuning pipeline, producing a model that‚Äôs not only capable of following instructions but also knows how to respond helpfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    prompt=full_prompt_format.format(system_prompt + EOS_TOKEN, \"Hello, how are you?\" + EOS_TOKEN)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üíæ Save Final Model Artifacts\n",
    "\n",
    "Now that our model has gone through pretraining ‚Üí SFT ‚Üí ORPO, we save all important artifacts:\n",
    "- config.json: The final ModelArgs, useful for reloading the model later.\n",
    "- tokenizer.model / tokenizer.json: Your tokenizer for inference or future fine-tuning.\n",
    "- Adapter weights from SFT and ORPO (if applicable).\n",
    "- A final README.md: Describes what the model is, how it was trained, and how to use it.\n",
    "\n",
    "This makes it ready for:\n",
    "- Upload to ü§ó Hugging Face Hub\n",
    "- Local inference and evaluation\n",
    "- Further fine-tuning or preference optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "readme_file = f\"\"\"---\n",
    "tags:\n",
    "- mlx\n",
    "- text-generation\n",
    "pipeline_tag: text-generation\n",
    "---\n",
    "\n",
    "# Large Language Model `{user_name}/{new_model_name}`\n",
    "\n",
    "This model was developed end-to-end on Apple Silicon using the [`mlx-lm-lora`](https://github.com/Goekdeniz-Guelmez/mlx-lm-lora) package, part of the **Creating LLMs from Scratch** course by G√∂kdeniz G√ºlmez.\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ About This Course\n",
    "\n",
    "This model is the result of a hands-on, full-stack training series on Apple Silicon covering:\n",
    "\n",
    "- Custom Qwen3 MoE model creation\n",
    "- Large-scale pretraining with web data\n",
    "- Supervised fine-tuning on rich conversational data\n",
    "- Advanced preference alignment via ORPO\n",
    "- Efficient local training and inference on Mac\n",
    "\n",
    "For full details, check out the official notebook:\n",
    "[Creating LLMs from Scratch ‚Äî Qwen3 MoE from Scratch](https://github.com/Goekdeniz-Guelmez/mlx-lm-lora/blob/main/examples/qwen3_moe_from_scratch.ipynb)\n",
    "\n",
    "Also visit the [`mlx-lm-lora` GitHub repository](https://github.com/Goekdeniz-Guelmez/mlx-lm-lora) for more tools and resources.\n",
    "\n",
    "---\n",
    "\n",
    "## üõ† Training Pipeline Overview\n",
    "\n",
    "1. **Pretraining**\n",
    "   Dataset: `{pretraining_dataset_name}` ({pretraining_dataset_samples or 'full dataset'})  \n",
    "   Description: Unsupervised training on cleaned, deduplicated web-scale data from FineWeb-200k.\n",
    "\n",
    "2. **Supervised Fine-Tuning (SFT)**\n",
    "   Dataset: `{finetuning_dataset_name}` ({finetuning_dataset_samples or 'full dataset'})  \n",
    "   Description: Instruction-following, conversational data from Wizard Vicuna 70k uncensored subset.\n",
    "\n",
    "3. **Preference Optimization**\n",
    "   Dataset: `{preference_dataset_name}` ({preference_dataset_samples or 'full dataset'})  \n",
    "   Description: Ranked prompt-completion pairs from Human-Like DPO dataset to align with human preferences.\n",
    "\n",
    "---\n",
    "\n",
    "## üìö About the Model\n",
    "\n",
    "| Field                 | Value                                                                                 |\n",
    "|-----------------------|---------------------------------------------------------------------------------------|\n",
    "| **Model Name**        | `{new_model_name}`                                                        |\n",
    "| **Architecture**      | {model.args.model_type}                                                    |\n",
    "| **Alignment Method**  | Monolithic Preference Optimization (ORPO)                                            |\n",
    "| **Training Framework**| [`mlx-lm-lora`](https://github.com/Goekdeniz-Guelmez/mlx-lm-lora) on Apple Silicon   |\n",
    "| **Author**            | G√∂kdeniz G√ºlmez                                                                      |\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Usage Example (Python)\n",
    "\n",
    "```python\n",
    "from mlx_lm.utils import load\n",
    "from mlx_lm import generate\n",
    "\n",
    "model, tokenizer = load(\"{user_name}/{new_model_name}\")\n",
    "\n",
    "generate(model, tokenizer, \"What is the meaning of life?\")\n",
    "\"\"\"\n",
    "\n",
    "new_readme_path = f\"{target_dir}/README.md\"\n",
    "with open(new_readme_path, \"w\") as new_readme_file:\n",
    "    new_readme_file.write(readme_file)\n",
    "\n",
    "save_model(target_dir, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = HfApi(token=hf_token)\n",
    "create_repo(\n",
    "  repo_id=f\"{user_name}/{new_model_name}\",\n",
    "  repo_type=\"model\",\n",
    "  exist_ok=True,\n",
    "  token=hf_token,\n",
    "  private=True\n",
    ")\n",
    "api.upload_folder(\n",
    "  folder_path=target_dir,\n",
    "  repo_id=f\"{user_name}/{new_model_name}\",\n",
    "  token=hf_token,\n",
    "  commit_message=\"Initial Commit\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü´∂ Thanks\n",
    "\n",
    "You‚Äôve made it! üéâ\n",
    "\n",
    "By the end of this notebook, you‚Äôve:\n",
    "- Pretrained a foundation model from scratch üí™\n",
    "- Supervised it on assistant-style instructions ‚úçÔ∏è\n",
    "- Aligned it using ranked preferences and ORPO ‚ù§Ô∏è‚Äçüî•\n",
    "- Saved, organized, and prepped your model for deployment or sharing üóÉÔ∏è\n",
    "\n",
    "Thank you for walking through this course ‚Äî and most importantly, for building helpful AI responsibly.\n",
    "Josie (and I!) are proud of you. Until next time!\n",
    "\n",
    "G√∂kdeniz G√ºlmez üëã"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
